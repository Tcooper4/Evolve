"""
XGBoost Model for Time Series Forecasting with Comprehensive Error Handling

This module provides an XGBoost-based model for time series forecasting
in the Evolve trading system with robust error handling and fallback mechanisms.
"""

import logging
import traceback
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, Optional, Tuple, List
import warnings

import joblib
import numpy as np
import pandas as pd
from sklearn.feature_selection import SelectKBest, f_regression, RFE
from sklearn.model_selection import cross_val_score, TimeSeriesSplit
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestRegressor

from .base_model import BaseModel
from utils.model_cache import cache_model_operation, get_model_cache
from utils.forecast_helpers import safe_forecast, validate_forecast_input, log_forecast_performance
from trading.exceptions import ModelTrainingError, ModelPredictionError, ModelInitializationError

logger = logging.getLogger(__name__)


class FallbackXGBoostModel:
    """Fallback model using Random Forest when XGBoost fails."""
    def __init__(self):
        self.model = RandomForestRegressor(n_estimators=100, random_state=42)
        self.scaler = StandardScaler()
        self.is_trained = False
        
    def fit(self, X: pd.DataFrame, y: pd.Series) -> None:
        """Train the fallback model."""
        try:
            X_scaled = self.scaler.fit_transform(X)
            self.model.fit(X_scaled, y)
            self.is_trained = True
            logger.info("Fallback Random Forest model trained successfully")
        except Exception as e:
            logger.error(f"Fallback model training failed: {e}")
            self.is_trained = False
            
    def predict(self, X: pd.DataFrame) -> np.ndarray:
        """Predict using the fallback model."""
        try:
            if not self.is_trained:
                raise ValueError("Fallback model not trained")
            X_scaled = self.scaler.transform(X)
            return self.model.predict(X_scaled)
        except Exception as e:
            logger.error(f"Fallback model prediction failed: {e}")
            # Return mean of training data or constant
            return np.full(len(X), 1000.0)

class XGBoostModel(BaseModel):
    """XGBoost-based time series forecasting model with comprehensive error handling."""
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        """Initialize XGBoost model with error handling."""
        try:
            super().__init__(config)

            # Default configuration
            self.config = config or {}
            # Load hyperparameters from config file or use defaults
            self.model_params = self._load_hyperparameters()

            self.model = None
            self.is_trained = False
            self.feature_names = None
            self.feature_selector = None
            self.scaler = StandardScaler()
            self.selected_features = None
            self.feature_importance_scores = None
            
            # Initialize fallback model
            self.fallback_model = FallbackXGBoostModel()
            
            # Setup model with error handling
            try:
                self._setup_model()
            except Exception as e:
                logger.error(f"XGBoost setup failed, will use fallback: {e}")
                self.model = None
                
            logger.info("XGBoost model initialized successfully")
            
        except Exception as e:
            logger.error(f"Failed to initialize XGBoost model: {e}")
            logger.error(traceback.format_exc())
            raise ModelInitializationError(f"XGBoost model initialization failed: {str(e)}")

    def _load_hyperparameters(self) -> Dict[str, Any]:
        """Load XGBoost hyperparameters with error handling."""
        try:
            import os
            import yaml

            # Default hyperparameters
            default_params = {
              "n_estimators": 100,
              "max_depth": 6,
                "learning_rate": 0.1,
                "subsample": 0.8,
               "colsample_bytree": 0.8,
                "random_state": 42,
                "objective": "reg:squarederror",
                "eval_metric": "rmse",
            }

            # Try to load from config file
            config_paths = [
             "config/xgboost_config.yaml",
               "config/models/xgboost.yaml",
                "trading/config/xgboost.yaml",    ]

            for config_path in config_paths:
                try:
                    if os.path.exists(config_path):
                        with open(config_path, "r") as f:
                            config_data = yaml.safe_load(f)
                            if config_data and "xgboost" in config_data:
                                logger.info(f"Loaded XGBoost config from {config_path}")
                                return {**default_params, **config_data["xgboost"]}
                except Exception as e:
                    logger.warning(f"Failed to load config from {config_path}: {e}")

            # Try to load from environment variables
            env_params = {}
            env_mappings = {
               "XGB_N_ESTIMATORS": "n_estimators",
                "XGB_MAX_DEPTH": "max_depth",
                "XGB_LEARNING_RATE": "learning_rate",
                "XGB_SUBSAMPLE": "subsample",
                "XGB_COLSAMPLE_BYTREE": "colsample_bytree",
               "XGB_RANDOM_STATE": "random_state",
            }

            for env_var, param_name in env_mappings.items():
                env_value = os.getenv(env_var)
                if env_value:
                    try:
                        # Convert to appropriate type
                        if param_name in ["n_estimators", "max_depth", "random_state"]:
                            env_params[param_name] = int(env_value)
                        else:
                            env_params[param_name] = float(env_value)
                    except ValueError:
                        logger.warning(f"Invalid value for {env_var}: {env_value}")

            if env_params:
                logger.info("Loaded XGBoost parameters from environment variables")
                return {**default_params, **env_params}

            logger.info("Using default XGBoost parameters")
            return default_params
            
        except Exception as e:
            logger.error(f"Failed to load hyperparameters: {e}")
            # Return safe defaults
            return {
                "n_estimators": 50,
              "max_depth": 4,
                "learning_rate": 0.1,
                "subsample": 0.8,
               "colsample_bytree": 0.8,
                "random_state": 42,
                "objective": "reg:squarederror",
                "eval_metric": "rmse",
            }

    def _setup_model(self):
        """Setup XGBoost model with error handling."""
        try:
            import xgboost as xgb

            # Use the loaded hyperparameters
            self.model = xgb.XGBRegressor(**self.model_params)
            logger.info(f"XGBoost model initialized with parameters: {self.model_params}")
            
        except ImportError as e:
            logger.error(f"XGBoost not available: {e}")
            raise ImportError("XGBoost is required for this model. Install with: pip install xgboost")
        except Exception as e:
            logger.error(f"Failed to setup XGBoost model: {e}")
            raise ModelInitializationError(f"XGBoost model setup failed: {str(e)}")

    def _create_lag_features(self, data: pd.DataFrame, max_lags: int = 20) -> pd.DataFrame:
        """Create lag features with comprehensive error handling."""
        try:
            if not isinstance(data, pd.DataFrame):
                raise ValueError("Input must be a pandas DataFrame")
                
            if data.empty:
                raise ValueError("Input data is empty")
                
            features = data.copy()
            
            # Create lag features for close price
            if "close" in data.columns:
                for lag in range(1, max_lags + 1):
                    features[f"close_lag_{lag}"] = data["close"].shift(lag)
            
            # Create lag features for volume
            if "volume" in data.columns:
                for lag in range(1, min(max_lags, 10) + 1):
                    features[f"volume_lag_{lag}"] = data["volume"].shift(lag)
            
            # Create rolling statistics
            if "close" in data.columns:
                for window in [5, 10, 20, 50]:
                    features[f"close_ma_{window}"] = data["close"].rolling(window=window).mean()
                    features[f"close_std_{window}"] = data["close"].rolling(window=window).std()
                    features[f"close_min_{window}"] = data["close"].rolling(window=window).min()
                    features[f"close_max_{window}"] = data["close"].rolling(window=window).max()
            
            # Create technical indicators
            features = self._add_technical_indicators(features)
            
            # Remove rows with NaN values
            features = features.dropna()
            
            if features.empty:
                raise ValueError("No valid features after removing NaN values")
                
            return features
            
        except Exception as e:
            logger.error(f"Error creating lag features: {e}")
            logger.error(traceback.format_exc())
            raise ModelPredictionError(f"Feature creation failed: {str(e)}")

    def _add_technical_indicators(self, data: pd.DataFrame) -> pd.DataFrame:
        """Add technical indicators with error handling."""
        try:
            if "close" in data.columns:
                # RSI
                data["rsi"] = self._calculate_rsi(data["close"])
                
                # MACD
                data["macd"], data["macd_signal"] = self._calculate_macd(data["close"])
                
                # Bollinger Bands
                data["bb_upper"], data["bb_middle"], data["bb_lower"] = self._calculate_bollinger_bands(data["close"])
                
                # Price changes
                data["price_change"] = data["close"].pct_change()
                data["price_change_2"] = data["close"].pct_change(2)
                data["price_change_5"] = data["close"].pct_change(5)
                
                # Volatility
                data["volatility"] = data["close"].rolling(window=20).std()
                
            if "volume" in data.columns:
                # Volume indicators
                data["volume_ma"] = data["volume"].rolling(window=20).mean()
                data["volume_ratio"] = data["volume"] / data["volume_ma"]
                
            return data
            
        except Exception as e:
            logger.error(f"Error adding technical indicators: {e}")
            # Return data without technical indicators
            return data

    def _calculate_rsi(self, prices: pd.Series, period: int = 14) -> pd.Series:
        """Calculate RSI with error handling."""
        try:
            delta = prices.diff()
            gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()
            loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()
            rs = gain / loss
            rsi = 100 - (100 / (1 + rs))
            return rsi
        except Exception as e:
            logger.error(f"RSI calculation failed: {e}")
            return pd.Series(50.0, index=prices.index)  # Neutral RSI

    def _calculate_macd(self, prices: pd.Series, fast: int = 12, slow: int = 26, signal: int = 9) -> Tuple[pd.Series, pd.Series]:
        """Calculate MACD with error handling."""
        try:
            ema_fast = prices.ewm(span=fast).mean()
            ema_slow = prices.ewm(span=slow).mean()
            macd = ema_fast - ema_slow
            macd_signal = macd.ewm(span=signal).mean()
            return macd, macd_signal
        except Exception as e:
            logger.error(f"MACD calculation failed: {e}")
            return pd.Series(0.0, index=prices.index), pd.Series(0.0, index=prices.index)

    def _calculate_bollinger_bands(self, prices: pd.Series, window: int = 20, std_dev: float = 2) -> Tuple[pd.Series, pd.Series, pd.Series]:
        """Calculate Bollinger Bands with error handling."""
        try:
            middle = prices.rolling(window=window).mean()
            std = prices.rolling(window=window).std()
            upper = middle + (std * std_dev)
            lower = middle - (std * std_dev)
            return upper, middle, lower
        except Exception as e:
            logger.error(f"Bollinger Bands calculation failed: {e}")
            return prices, prices, prices

    def _select_features_cross_validation(self, X: pd.DataFrame, y: pd.Series, max_features: int = 50) -> List[str]:
        """Select features using cross-validation with error handling."""
        try:
            if len(X.columns) <= max_features:
                return list(X.columns)
                
            # Use correlation-based selection as fallback
            correlations = X.corrwith(y).abs().sort_values(ascending=False)
            selected_features = correlations.head(max_features).index.tolist()
            
            logger.info(f"Selected {len(selected_features)} features using correlation")
            return selected_features
            
        except Exception as e:
            logger.error(f"Feature selection failed: {e}")
            # Return first max_features columns
            return list(X.columns[:max_features])

    def prepare_features(self, data: pd.DataFrame) -> Tuple[pd.DataFrame, pd.Series]:
        """Prepare features with comprehensive error handling."""
        try:
            if not isinstance(data, pd.DataFrame):
                raise ValueError("Input must be a pandas DataFrame")
                
            if data.empty:
                raise ValueError("Input data is empty")
                
            # Check for required columns
            if "close" not in data.columns:
                raise ValueError("Data must contain 'close' column")
                
            # Handle NaN values
            data = data.fillna(method='ffill').fillna(method='bfill')
            
            # Create features
            features = self._create_lag_features(data)
            
            # Prepare target variable
            target = features["close"].shift(-1).dropna()
            features = features[:-1] # Remove last row since target is shifted
            
            # Align features and target
            common_index = features.index.intersection(target.index)
            features = features.loc[common_index]
            target = target.loc[common_index]
            
            if features.empty or target.empty:
                raise ValueError("No valid data after feature preparation")
                
            return features, target
            
        except Exception as e:
            logger.error(f"Feature preparation failed: {e}")
            logger.error(traceback.format_exc())
            raise ModelPredictionError(f"Feature preparation failed: {str(e)}")

    def train(self, data: pd.DataFrame) -> Dict[str, Any]:
        """Train the model with comprehensive error handling."""
        try:
            # Prepare features
            features, target = self.prepare_features(data)
            
            # Select features
            self.selected_features = self._select_features_cross_validation(features, target)
            X = features[self.selected_features]
            
            # Scale features
            X_scaled = self.scaler.fit_transform(X)
            
            # Train model
            if self.model is not None:
                try:
                    self.model.fit(X_scaled, target)
                    self.is_trained = True
                    self.feature_names = self.selected_features
                    
                    # Get feature importance
                    self.feature_importance_scores = dict(zip(
                        self.selected_features, 
                        self.model.feature_importances_
                    ))
                    
                    logger.info("XGBoost model trained successfully")
                    
                except Exception as e:
                    logger.error(f"XGBoost training failed: {e}")
                    logger.info("Using fallback model")
                    self.fallback_model.fit(X, target)
                    self.is_trained = True
                    
            else:
                # Use fallback model
                self.fallback_model.fit(X, target)
                self.is_trained = True
                
            return {
                "success": True,
                "model_type": "XGBoost" if self.model is not None else "RandomForest",
                "features_used": len(self.selected_features),
                "training_samples": len(X)
            }
            
        except Exception as e:
            logger.error(f"Model training failed: {e}")
            logger.error(traceback.format_exc())
            raise ModelTrainingError(f"Model training failed: {str(e)}")

    def predict(self, data: pd.DataFrame) -> np.ndarray:
        """Predict with comprehensive error handling."""
        try:
            if not self.is_trained:
                raise ValueError("Model must be trained before prediction")
                
            # Prepare features
            features, _ = self.prepare_features(data)
            
            if self.selected_features is None:
                raise ValueError("No features selected during training")
                
            X = features[self.selected_features]
            
            # Make prediction
            if self.model is not None and self.is_trained:
                try:
                    X_scaled = self.scaler.transform(X)
                    predictions = self.model.predict(X_scaled)
                    return predictions
                except Exception as e:
                    logger.error(f"XGBoost prediction failed: {e}")
                    logger.info("Using fallback model for prediction")
                    return self.fallback_model.predict(X)
            else:
                return self.fallback_model.predict(X)
                
        except Exception as e:
            logger.error(f"Prediction failed: {e}")
            logger.error(traceback.format_exc())
            
            # Return simple fallback prediction
            logger.info("Using simple fallback prediction")
            if "close" in data.columns:
                return data["close"].rolling(window=20).mean().values
            else:
                return np.full(len(data), 1000.0)

    def get_feature_importance(self) -> Dict[str, float]:
        """Feature importance with error handling."""
        try:
            if self.feature_importance_scores is not None:
                return self.feature_importance_scores
            else:
                return {}
        except Exception as e:
            logger.error(f"Failed to get feature importance: {e}")
            return {}

    def save(self, filepath: str) -> bool:
        """Save model with error handling."""
        try:
            # Create directory if it doesn't exist
            Path(filepath).parent.mkdir(parents=True, exist_ok=True)
            
            # Save model and metadata
            model_data = {
           "model": self.model,
               "scaler": self.scaler,
                "selected_features": self.selected_features,
       "feature_importance": self.feature_importance_scores,
            "config": self.config,
             "model_params": self.model_params,
               "fallback_model": self.fallback_model
            }
            
            joblib.dump(model_data, filepath)
            logger.info(f"Model saved successfully to {filepath}")
            return True          
        except Exception as e:
            logger.error(f"Failed to save model: {e}")
            logger.error(traceback.format_exc())
            return False

    def load(self, filepath: str) -> bool:
        """Load model with error handling."""
        try:
            if not Path(filepath).exists():
                raise FileNotFoundError(f"Model file not found: {filepath}")
                
            # Load model and metadata
            model_data = joblib.load(filepath)
            
            self.model = model_data.get("model")
            self.scaler = model_data.get("scaler", StandardScaler())
            self.selected_features = model_data.get("selected_features")
            self.feature_importance_scores = model_data.get("feature_importance")
            self.config = model_data.get("config", {})
            self.model_params = model_data.get("model_params", {})
            self.fallback_model = model_data.get("fallback_model", FallbackXGBoostModel())
            
            self.is_trained = True
            logger.info(f"Model loaded successfully from {filepath}")
            return True          
        except Exception as e:
            logger.error(f"Failed to load model: {e}")
            logger.error(traceback.format_exc())
            return False

    def get_metadata(self) -> Dict[str, Any]:
        """Get model metadata with error handling."""
        try:
            return {
                "model_type": "XGBoost",
                "is_trained": self.is_trained,
                "feature_count": len(self.selected_features) if self.selected_features else 0,
                "config": self.config,
                "model_params": self.model_params,
                "feature_importance": self.get_feature_importance()
            }
        except Exception as e:
            logger.error(f"Failed to get metadata: {e}")
            return {"error": str(e)}

    @cache_model_operation
    @safe_forecast(max_retries=2, retry_delay=0.5, log_errors=True)
    def forecast(self, data: pd.DataFrame, horizon: int = 30) -> Dict[str, Any]:
        """Generate forecast with comprehensive error handling."""
        try:
            # Validate inputs
            if data.empty:
                raise ModelPredictionError("Input data is empty")
                
            if horizon <= 0:
                raise ValueError("Forecast horizon must be positive")
                
            if not self.is_trained:
                raise ModelPredictionError("Model must be trained before forecasting")
                
            # Generate forecast
            try:
                # Use recursive forecasting
                current_data = data.copy()
                forecasts = []
                
                for _ in range(horizon):
                    # Get prediction for next step
                    pred = self.predict(current_data)
                    forecasts.append(pred[-1])
                    
                    # Update data for next iteration
                    new_row = current_data.iloc[-1].copy()
                    new_row["close"] = pred[-1]
                    current_data = pd.concat([current_data, pd.DataFrame([new_row])], ignore_index=True)
                    current_data = current_data.iloc[1:]  # Remove oldest row
                    
                # Create forecast dates
                last_date = data.index[-1] if hasattr(data.index[-1], 'freq') else pd.Timestamp.now()
                forecast_dates = pd.date_range(start=last_date, periods=horizon+1, freq='D')[1:]
                
                return {
                    "forecast": np.array(forecasts),
                    "dates": forecast_dates,
                    "confidence": np.full(horizon, 0.8),
                    "model_type": "XGBoost",
                    "horizon": horizon
                }
                
            except Exception as e:
                logger.error(f"Forecast generation failed: {e}")
                logger.error(traceback.format_exc())
                
                # Return fallback forecast
                logger.info("Using fallback forecast")
                if "close" in data.columns:
                    last_value = data["close"].iloc[-1]
                    trend = data["close"].diff().mean()
                    fallback_forecast = [last_value + trend * (i+1) for i in range(horizon)]
                else:
                    fallback_forecast = np.full(horizon, 1000.0)
                    
                last_date = data.index[-1] if hasattr(data.index[-1], 'freq') else pd.Timestamp.now()
                forecast_dates = pd.date_range(start=last_date, periods=horizon+1, freq='D')[1:]
                
                return {
                  "forecast": fallback_forecast,
                   "dates": forecast_dates,
                  "confidence": np.full(horizon, 0.5),
                    "model_type": "XGBoost_Fallback",
                    "horizon": horizon
                }
                
        except Exception as e:
            logger.error(f"XGBoost forecast failed: {e}")
            logger.error(traceback.format_exc())
            
            # Return simple fallback
            logger.info("Using simple fallback forecast")
            fallback_forecast = np.full(horizon, 10.0)
            last_date = data.index[-1] if hasattr(data.index[-1], 'freq') else pd.Timestamp.now()
            forecast_dates = pd.date_range(start=last_date, periods=horizon+1, freq='D')[1:]
            
            return {
              "forecast": fallback_forecast,
               "dates": forecast_dates,
              "confidence": np.full(horizon, 0.3),
                "model_type": "XGBoost_Simple_Fallback",
                "horizon": horizon
            }
