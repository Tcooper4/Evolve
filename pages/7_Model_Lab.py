"""
Model Laboratory Page

Merges functionality from:
- Model_Lab.py
- 6_Model_Optimization.py
- Model_Performance_Dashboard.py
- 7_Optimizer.py
- 8_Explainability.py

Features:
- Quick model training
- Detailed model configuration
- Hyperparameter optimization
- Model performance tracking
- Model comparison
- Model explainability (SHAP, LIME)
- Model registry and versioning
"""

import logging
import sys
from datetime import datetime, timedelta
from typing import Dict, List, Optional

import numpy as np
import pandas as pd
import plotly.graph_objects as go
import streamlit as st

# Backend imports
try:
    # Model imports
    from trading.models.lstm_model import LSTMModel
    from trading.models.xgboost_model import XGBoostModel
    from trading.models.prophet_model import ProphetModel
    from trading.models.arima_model import ARIMAModel
    
    # Data loading
    from trading.data.data_loader import DataLoader, DataLoadRequest
    from trading.data.providers.yfinance_provider import YFinanceProvider
    
    # Optimization and evaluation
    from trading.optimization.optuna_tuner import OptunaTuner
    from trading.evaluation.model_evaluator import ModelEvaluator
    
    # Explainability
    from trading.analytics.forecast_explainability import ForecastExplainability
    
    # Model registry
    from trading.integration.model_registry import ModelRegistry
    
    MODEL_MODULES_AVAILABLE = True
except ImportError as e:
    logging.warning(f"Some model modules not available: {e}")
    MODEL_MODULES_AVAILABLE = False

# Setup logging
logger = logging.getLogger(__name__)

# Page config
st.set_page_config(
    page_title="Model Laboratory",
    page_icon="üî¨",
    layout="wide"
)

# Initialize session state
if 'model_registry' not in st.session_state:
    try:
        st.session_state.model_registry = ModelRegistry() if MODEL_MODULES_AVAILABLE else None
    except Exception as e:
        logger.warning(f"Could not initialize model registry: {e}")
        st.session_state.model_registry = None

if 'model_evaluator' not in st.session_state:
    try:
        st.session_state.model_evaluator = ModelEvaluator() if MODEL_MODULES_AVAILABLE else None
    except Exception as e:
        logger.warning(f"Could not initialize model evaluator: {e}")
        st.session_state.model_evaluator = None

if 'optuna_tuner' not in st.session_state:
    try:
        st.session_state.optuna_tuner = OptunaTuner() if MODEL_MODULES_AVAILABLE else None
    except Exception as e:
        logger.warning(f"Could not initialize Optuna tuner: {e}")
        st.session_state.optuna_tuner = None

if 'explainability' not in st.session_state:
    try:
        st.session_state.explainability = ForecastExplainability() if MODEL_MODULES_AVAILABLE else None
    except Exception as e:
        logger.warning(f"Could not initialize explainability engine: {e}")
        st.session_state.explainability = None

if 'trained_models' not in st.session_state:
    st.session_state.trained_models = {}

if 'training_data' not in st.session_state:
    st.session_state.training_data = None

if 'data_loader' not in st.session_state:
    try:
        # Try to initialize DataLoader - it may have different constructors
        try:
            from trading.data.providers.provider_manager import ProviderManager
            provider_manager = ProviderManager()
            st.session_state.data_loader = DataLoader(provider_manager) if MODEL_MODULES_AVAILABLE else None
        except Exception as e:
            # Fallback: try direct instantiation
            st.session_state.data_loader = DataLoader() if MODEL_MODULES_AVAILABLE else None
    except Exception as e:
        logger.warning(f"Could not initialize data loader: {e}")
        st.session_state.data_loader = None

if 'quick_training_results' not in st.session_state:
    st.session_state.quick_training_results = {}

# Main page title
st.title("üî¨ Model Laboratory")
st.markdown("Train, optimize, evaluate, and deploy machine learning models for trading")

st.markdown("---")

# Create tabs
tab1, tab2, tab3, tab4, tab5, tab6, tab7 = st.tabs([
    "‚ö° Quick Training",
    "‚öôÔ∏è Model Configuration",
    "üéØ Hyperparameter Optimization",
    "üìä Model Performance",
    "üîç Model Comparison",
    "üß† Explainability",
    "üìö Model Registry"
])

# TAB 1: Quick Training
with tab1:
    st.header("‚ö° Quick Training")
    st.markdown("Fast model training with default parameters. Perfect for quick experiments and prototyping.")
    
    # Data Selection Section
    st.subheader("üìä Data Selection")
    
    data_source = st.radio(
        "Data Source",
        ["Load from Market", "Upload CSV File", "Use Previous Data"],
        horizontal=True
    )
    
    data = None
    
    if data_source == "Load from Market":
        col1, col2, col3 = st.columns(3)
        
        with col1:
            symbol = st.text_input(
                "Ticker Symbol",
                value="AAPL",
                help="Enter stock ticker (e.g., AAPL, MSFT, GOOGL)"
            ).upper()
        
        with col2:
            start_date = st.date_input(
                "Start Date",
                value=datetime.now() - pd.Timedelta(days=365),
                max_value=datetime.now()
            )
        
        with col3:
            end_date = st.date_input(
                "End Date",
                value=datetime.now(),
                max_value=datetime.now()
            )
        
        if st.button("üì• Load Data", type="primary"):
            if st.session_state.data_loader:
                try:
                    with st.spinner(f"Loading data for {symbol}..."):
                        request = DataLoadRequest(
                            ticker=symbol,
                            start_date=start_date.strftime("%Y-%m-%d"),
                            end_date=end_date.strftime("%Y-%m-%d"),
                            interval="1d"
                        )
                        response = st.session_state.data_loader.load_market_data(request)
                        
                        if response.success and response.data is not None:
                            data = response.data
                            st.session_state.training_data = data
                            st.success(f"‚úÖ Loaded {len(data)} records for {symbol}")
                        else:
                            st.error(f"‚ùå Failed to load data: {response.message}")
                except Exception as e:
                    st.error(f"Error loading data: {str(e)}")
            else:
                st.error("Data loader not available. Please check backend configuration.")
    
    elif data_source == "Upload CSV File":
        uploaded_file = st.file_uploader(
            "Upload CSV File",
            type=['csv'],
            help="Upload a CSV file with OHLCV data. Must include 'Close' or 'close' column."
        )
        
        if uploaded_file is not None:
            try:
                data = pd.read_csv(uploaded_file)
                
                # Try to parse date column
                date_columns = ['date', 'Date', 'timestamp', 'Timestamp', 'time', 'Time']
                for col in date_columns:
                    if col in data.columns:
                        data[col] = pd.to_datetime(data[col])
                        data.set_index(col, inplace=True)
                        break
                
                # Ensure we have a close price column
                close_col = None
                for col in ['Close', 'close', 'CLOSE', 'price', 'Price']:
                    if col in data.columns:
                        close_col = col
                        break
                
                if close_col is None:
                    st.error("‚ùå CSV must contain a 'Close' or 'close' column")
                else:
                    st.session_state.training_data = data
                    st.success(f"‚úÖ Loaded {len(data)} records from CSV")
                    data = st.session_state.training_data
            except Exception as e:
                st.error(f"Error reading CSV: {str(e)}")
    
    elif data_source == "Use Previous Data":
        if st.session_state.training_data is not None:
            data = st.session_state.training_data
            st.info(f"‚úÖ Using previous data: {len(data)} records")
        else:
            st.warning("‚ö†Ô∏è No previous data available. Please load data first.")
    
    # Model Selection Section
    if data is not None or st.session_state.training_data is not None:
        if data is None:
            data = st.session_state.training_data
        
        st.markdown("---")
        st.subheader("ü§ñ Model Selection")
        
        model_type = st.selectbox(
            "Model Type",
            ["LSTM", "XGBoost", "Prophet", "ARIMA"],
            help="Select the model type to train"
        )
        
        # Display data preview
        with st.expander("üìã Data Preview", expanded=False):
            st.dataframe(data.head(10))
            st.caption(f"Total records: {len(data)}")
            st.caption(f"Date range: {data.index[0]} to {data.index[-1]}")
        
        # Training Configuration
        st.markdown("---")
        st.subheader("‚öôÔ∏è Training Configuration")
        
        col1, col2 = st.columns(2)
        
        with col1:
            target_column = st.selectbox(
                "Target Column",
                options=[col for col in data.columns if col.lower() in ['close', 'price', 'adj close']] or ['Close'],
                help="Column to use as target for prediction"
            )
            
            train_test_split = st.slider(
                "Train/Test Split",
                min_value=0.7,
                max_value=0.95,
                value=0.8,
                step=0.05,
                help="Percentage of data to use for training"
            )
        
        with col2:
            forecast_horizon = st.number_input(
                "Forecast Horizon (days)",
                min_value=1,
                max_value=30,
                value=7,
                help="Number of days to forecast"
            )
            
            model_name = st.text_input(
                "Model Name (optional)",
                value=f"{model_type}_{symbol if 'symbol' in locals() else 'model'}_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                help="Name for saving the trained model"
            )
        
        # Train Button
        st.markdown("---")
        train_button = st.button("üöÄ Train Model", type="primary", use_container_width=True)
        
        if train_button:
            try:
                # Prepare data
                if target_column not in data.columns:
                    st.error(f"‚ùå Target column '{target_column}' not found in data")
                else:
                    # Get target series
                    target_data = data[target_column].dropna()
                    
                    if len(target_data) < 10:
                        st.error("‚ùå Insufficient data for training (need at least 10 records)")
                    else:
                        # Split data
                        split_idx = int(len(target_data) * train_test_split)
                        train_data = target_data[:split_idx]
                        test_data = target_data[split_idx:]
                        
                        # Initialize progress
                        progress_bar = st.progress(0)
                        status_text = st.empty()
                        
                        # Initialize model with default parameters
                        status_text.text(f"Initializing {model_type} model...")
                        progress_bar.progress(0.1)
                        
                        model = None
                        model_config = {}
                        
                        if model_type == "LSTM":
                            model_config = {
                                "input_dim": 1,
                                "hidden_dim": 64,
                                "num_layers": 2,
                                "output_dim": 1,
                                "dropout": 0.2,
                                "learning_rate": 0.001
                            }
                            model = LSTMModel(**model_config)
                        
                        elif model_type == "XGBoost":
                            model_config = {
                                "n_estimators": 100,
                                "max_depth": 5,
                                "learning_rate": 0.1,
                                "subsample": 0.8
                            }
                            model = XGBoostModel(**model_config)
                        
                        elif model_type == "Prophet":
                            model_config = {
                                "changepoint_prior_scale": 0.05,
                                "seasonality_prior_scale": 10.0,
                                "seasonality_mode": "additive"
                            }
                            model = ProphetModel(**model_config)
                        
                        elif model_type == "ARIMA":
                            model_config = {
                                "order": (5, 1, 0),
                                "use_auto_arima": True
                            }
                            model = ARIMAModel(**model_config)
                        
                        if model is None:
                            st.error(f"‚ùå Failed to initialize {model_type} model")
                        else:
                            # Train model
                            status_text.text(f"Training {model_type} model...")
                            progress_bar.progress(0.3)
                            
                            try:
                                if model_type == "Prophet":
                                    # Prophet requires specific format
                                    train_df = pd.DataFrame({
                                        'ds': train_data.index,
                                        'y': train_data.values
                                    })
                                    model.fit(train_df)
                                elif model_type == "ARIMA":
                                    model.fit(train_data.values)
                                else:
                                    # LSTM and XGBoost
                                    X_train = train_data.values.reshape(-1, 1)
                                    y_train = train_data.values
                                    model.fit(X_train, y_train)
                                
                                progress_bar.progress(0.7)
                                status_text.text("Evaluating model...")
                                
                                # Evaluate on test set
                                if model_type == "Prophet":
                                    test_df = pd.DataFrame({
                                        'ds': test_data.index,
                                        'y': test_data.values
                                    })
                                    predictions = model.predict(test_df)
                                    y_pred = predictions['yhat'].values if 'yhat' in predictions.columns else predictions.values.flatten()
                                elif model_type == "ARIMA":
                                    predictions = model.predict(len(test_data))
                                    y_pred = predictions
                                else:
                                    X_test = test_data.values.reshape(-1, 1)
                                    y_pred = model.predict(X_test)
                                
                                # Calculate metrics
                                y_true = test_data.values
                                
                                # Ensure same length
                                min_len = min(len(y_true), len(y_pred))
                                y_true = y_true[:min_len]
                                y_pred = y_pred[:min_len]
                                
                                # Calculate metrics
                                mse = np.mean((y_true - y_pred) ** 2)
                                mae = np.mean(np.abs(y_true - y_pred))
                                rmse = np.sqrt(mse)
                                mape = np.mean(np.abs((y_true - y_pred) / (y_true + 1e-8))) * 100
                                
                                # R-squared
                                ss_res = np.sum((y_true - y_pred) ** 2)
                                ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)
                                r2 = 1 - (ss_res / (ss_tot + 1e-8))
                                
                                progress_bar.progress(1.0)
                                status_text.text("‚úÖ Training complete!")
                                
                                # Store results
                                training_result = {
                                    "model": model,
                                    "model_type": model_type,
                                    "model_config": model_config,
                                    "model_name": model_name,
                                    "metrics": {
                                        "MSE": mse,
                                        "MAE": mae,
                                        "RMSE": rmse,
                                        "MAPE": mape,
                                        "R¬≤": r2
                                    },
                                    "train_size": len(train_data),
                                    "test_size": len(test_data),
                                    "trained_at": datetime.now().isoformat()
                                }
                                
                                st.session_state.quick_training_results[model_name] = training_result
                                
                                # Display results
                                st.success("‚úÖ Model trained successfully!")
                                
                                st.markdown("---")
                                st.subheader("üìä Training Results")
                                
                                # Metrics display
                                col1, col2, col3, col4, col5 = st.columns(5)
                                
                                with col1:
                                    st.metric("RMSE", f"{rmse:.4f}")
                                
                                with col2:
                                    st.metric("MAE", f"{mae:.4f}")
                                
                                with col3:
                                    st.metric("MAPE", f"{mape:.2f}%")
                                
                                with col4:
                                    st.metric("R¬≤ Score", f"{r2:.4f}")
                                
                                with col5:
                                    st.metric("MSE", f"{mse:.4f}")
                                
                                # Prediction vs Actual plot
                                fig = go.Figure()
                                
                                # Actual values
                                fig.add_trace(go.Scatter(
                                    x=list(range(len(y_true))),
                                    y=y_true,
                                    mode='lines',
                                    name='Actual',
                                    line=dict(color='blue', width=2)
                                ))
                                
                                # Predictions
                                fig.add_trace(go.Scatter(
                                    x=list(range(len(y_pred))),
                                    y=y_pred,
                                    mode='lines',
                                    name='Predicted',
                                    line=dict(color='red', width=2, dash='dash')
                                ))
                                
                                fig.update_layout(
                                    title="Prediction vs Actual (Test Set)",
                                    xaxis_title="Time Step",
                                    yaxis_title="Value",
                                    hovermode='x unified',
                                    height=400
                                )
                                
                                st.plotly_chart(fig, use_container_width=True)
                                
                                # Save model button
                                st.markdown("---")
                                col_save1, col_save2 = st.columns([1, 1])
                                
                                with col_save1:
                                    if st.button("üíæ Save Model", use_container_width=True):
                                        try:
                                            if st.session_state.model_registry:
                                                # Save to registry
                                                st.session_state.model_registry.save_model(
                                                    model_name=model_name,
                                                    model=model,
                                                    metadata={
                                                        "model_type": model_type,
                                                        "config": model_config,
                                                        "metrics": training_result["metrics"],
                                                        "trained_at": training_result["trained_at"]
                                                    }
                                                )
                                                st.success(f"‚úÖ Model '{model_name}' saved to registry!")
                                            else:
                                                # Store in session state
                                                if 'saved_models' not in st.session_state:
                                                    st.session_state.saved_models = {}
                                                st.session_state.saved_models[model_name] = training_result
                                                st.success(f"‚úÖ Model '{model_name}' saved to session!")
                                        except Exception as e:
                                            st.error(f"Error saving model: {str(e)}")
                                
                                with col_save2:
                                    if st.button("üîÑ Train Another Model", use_container_width=True):
                                        st.rerun()
                                
                            except Exception as e:
                                st.error(f"‚ùå Training failed: {str(e)}")
                                logger.exception("Training error")
                            
                            finally:
                                progress_bar.empty()
                                status_text.empty()
            
            except Exception as e:
                st.error(f"‚ùå Error: {str(e)}")
                logger.exception("Quick training error")
    
    else:
        st.info("üëÜ Please load or upload data to begin training.")

# TAB 2: Model Configuration
with tab2:
    st.header("‚öôÔ∏è Model Configuration")
    st.markdown("Advanced model configuration with full control over architecture, hyperparameters, and feature engineering.")
    
    # Data Selection (reuse from Tab 1 if available)
    st.subheader("üìä Data Selection")
    
    if st.session_state.training_data is not None:
        data = st.session_state.training_data
        st.info(f"‚úÖ Using data from Quick Training: {len(data)} records")
        use_existing_data = True
    else:
        use_existing_data = False
        col1, col2, col3 = st.columns(3)
        
        with col1:
            symbol = st.text_input("Ticker Symbol", value="AAPL").upper()
        
        with col2:
            start_date = st.date_input(
                "Start Date",
                value=datetime.now() - timedelta(days=365),
                max_value=datetime.now()
            )
        
        with col3:
            end_date = st.date_input(
                "End Date",
                value=datetime.now(),
                max_value=datetime.now()
            )
        
        if st.button("üì• Load Data"):
            if st.session_state.data_loader:
                try:
                    with st.spinner(f"Loading data for {symbol}..."):
                        request = DataLoadRequest(
                            ticker=symbol,
                            start_date=start_date.strftime("%Y-%m-%d"),
                            end_date=end_date.strftime("%Y-%m-%d"),
                            interval="1d"
                        )
                        response = st.session_state.data_loader.load_market_data(request)
                        
                        if response.success and response.data is not None:
                            data = response.data
                            st.session_state.training_data = data
                            use_existing_data = True
                            st.success(f"‚úÖ Loaded {len(data)} records")
                        else:
                            st.error(f"‚ùå Failed to load data: {response.message}")
                except Exception as e:
                    st.error(f"Error loading data: {str(e)}")
            else:
                st.error("Data loader not available")
    
    if use_existing_data or st.session_state.training_data is not None:
        if not use_existing_data:
            data = st.session_state.training_data
        
        st.markdown("---")
        
        # Model Type Selection
        st.subheader("ü§ñ Model Architecture")
        
        model_type = st.selectbox(
            "Model Type",
            ["LSTM", "XGBoost", "Prophet", "ARIMA"],
            help="Select the model type"
        )
        
        # Model-Specific Configuration
        model_config = {}
        
        if model_type == "LSTM":
            st.markdown("**LSTM Architecture Configuration**")
            
            col1, col2 = st.columns(2)
            
            with col1:
                model_config['input_dim'] = st.number_input(
                    "Input Dimension",
                    min_value=1,
                    max_value=100,
                    value=1,
                    help="Number of input features"
                )
                
                model_config['hidden_dim'] = st.number_input(
                    "Hidden Dimension",
                    min_value=16,
                    max_value=512,
                    value=64,
                    step=16,
                    help="Size of LSTM hidden layers"
                )
                
                model_config['num_layers'] = st.number_input(
                    "Number of Layers",
                    min_value=1,
                    max_value=5,
                    value=2,
                    help="Number of LSTM layers"
                )
                
                model_config['sequence_length'] = st.number_input(
                    "Sequence Length",
                    min_value=5,
                    max_value=100,
                    value=60,
                    help="Length of input sequences"
                )
            
            with col2:
                model_config['dropout'] = st.slider(
                    "Dropout Rate",
                    min_value=0.0,
                    max_value=0.5,
                    value=0.2,
                    step=0.05,
                    help="Dropout rate for regularization"
                )
                
                model_config['learning_rate'] = st.number_input(
                    "Learning Rate",
                    min_value=0.0001,
                    max_value=0.01,
                    value=0.001,
                    step=0.0001,
                    format="%.4f",
                    help="Learning rate for optimizer"
                )
                
                model_config['batch_size'] = st.number_input(
                    "Batch Size",
                    min_value=8,
                    max_value=256,
                    value=32,
                    step=8,
                    help="Training batch size"
                )
                
                model_config['epochs'] = st.number_input(
                    "Epochs",
                    min_value=10,
                    max_value=500,
                    value=100,
                    step=10,
                    help="Number of training epochs"
                )
            
            # Additional LSTM options
            with st.expander("üîß Advanced LSTM Options"):
                col1, col2 = st.columns(2)
                
                with col1:
                    model_config['bidirectional'] = st.checkbox(
                        "Bidirectional LSTM",
                        value=False,
                        help="Use bidirectional LSTM layers"
                    )
                    
                    model_config['use_batch_norm'] = st.checkbox(
                        "Batch Normalization",
                        value=False,
                        help="Apply batch normalization"
                    )
                
                with col2:
                    model_config['use_layer_norm'] = st.checkbox(
                        "Layer Normalization",
                        value=False,
                        help="Apply layer normalization"
                    )
                    
                    model_config['additional_dropout'] = st.slider(
                        "Additional Dropout",
                        min_value=0.0,
                        max_value=0.5,
                        value=0.0,
                        step=0.05
                    )
        
        elif model_type == "XGBoost":
            st.markdown("**XGBoost Configuration**")
            
            col1, col2 = st.columns(2)
            
            with col1:
                model_config['n_estimators'] = st.number_input(
                    "Number of Estimators",
                    min_value=50,
                    max_value=1000,
                    value=100,
                    step=50,
                    help="Number of boosting rounds"
                )
                
                model_config['max_depth'] = st.number_input(
                    "Max Depth",
                    min_value=3,
                    max_value=20,
                    value=6,
                    help="Maximum tree depth"
                )
                
                model_config['learning_rate'] = st.number_input(
                    "Learning Rate",
                    min_value=0.01,
                    max_value=0.3,
                    value=0.1,
                    step=0.01,
                    help="Boosting learning rate"
                )
                
                model_config['subsample'] = st.slider(
                    "Subsample",
                    min_value=0.5,
                    max_value=1.0,
                    value=0.8,
                    step=0.1,
                    help="Subsample ratio of training instances"
                )
            
            with col2:
                model_config['colsample_bytree'] = st.slider(
                    "Column Sample by Tree",
                    min_value=0.5,
                    max_value=1.0,
                    value=1.0,
                    step=0.1,
                    help="Subsample ratio of columns"
                )
                
                model_config['min_child_weight'] = st.number_input(
                    "Min Child Weight",
                    min_value=1,
                    max_value=10,
                    value=1,
                    help="Minimum sum of instance weight in a child"
                )
                
                model_config['gamma'] = st.number_input(
                    "Gamma (Regularization)",
                    min_value=0.0,
                    max_value=5.0,
                    value=0.0,
                    step=0.1,
                    help="Minimum loss reduction for split"
                )
                
                model_config['reg_alpha'] = st.number_input(
                    "L1 Regularization (Alpha)",
                    min_value=0.0,
                    max_value=10.0,
                    value=0.0,
                    step=0.1,
                    help="L1 regularization term"
                )
                
                model_config['reg_lambda'] = st.number_input(
                    "L2 Regularization (Lambda)",
                    min_value=0.0,
                    max_value=10.0,
                    value=1.0,
                    step=0.1,
                    help="L2 regularization term"
                )
        
        elif model_type == "Prophet":
            st.markdown("**Prophet Configuration**")
            
            col1, col2 = st.columns(2)
            
            with col1:
                model_config['changepoint_prior_scale'] = st.number_input(
                    "Changepoint Prior Scale",
                    min_value=0.001,
                    max_value=0.5,
                    value=0.05,
                    step=0.01,
                    format="%.3f",
                    help="Flexibility of changepoints"
                )
                
                model_config['seasonality_prior_scale'] = st.number_input(
                    "Seasonality Prior Scale",
                    min_value=0.01,
                    max_value=50.0,
                    value=10.0,
                    step=1.0,
                    help="Strength of seasonality"
                )
                
                model_config['holidays_prior_scale'] = st.number_input(
                    "Holidays Prior Scale",
                    min_value=0.01,
                    max_value=50.0,
                    value=10.0,
                    step=1.0,
                    help="Strength of holiday effects"
                )
            
            with col2:
                model_config['seasonality_mode'] = st.selectbox(
                    "Seasonality Mode",
                    ["additive", "multiplicative"],
                    help="Type of seasonality"
                )
                
                model_config['yearly_seasonality'] = st.checkbox(
                    "Yearly Seasonality",
                    value=True,
                    help="Fit yearly seasonality"
                )
                
                model_config['weekly_seasonality'] = st.checkbox(
                    "Weekly Seasonality",
                    value=True,
                    help="Fit weekly seasonality"
                )
                
                model_config['daily_seasonality'] = st.checkbox(
                    "Daily Seasonality",
                    value=False,
                    help="Fit daily seasonality"
                )
        
        elif model_type == "ARIMA":
            st.markdown("**ARIMA Configuration**")
            
            col1, col2 = st.columns(2)
            
            with col1:
                model_config['p'] = st.number_input(
                    "AR Order (p)",
                    min_value=0,
                    max_value=10,
                    value=5,
                    help="Autoregressive order"
                )
                
                model_config['d'] = st.number_input(
                    "I Order (d)",
                    min_value=0,
                    max_value=3,
                    value=1,
                    help="Differencing order"
                )
                
                model_config['q'] = st.number_input(
                    "MA Order (q)",
                    min_value=0,
                    max_value=10,
                    value=0,
                    help="Moving average order"
                )
            
            with col2:
                model_config['use_auto_arima'] = st.checkbox(
                    "Use Auto ARIMA",
                    value=True,
                    help="Automatically find best (p,d,q) parameters"
                )
                
                model_config['seasonal'] = st.checkbox(
                    "Seasonal ARIMA",
                    value=False,
                    help="Use seasonal ARIMA (SARIMA)"
                )
                
                if model_config['seasonal']:
                    model_config['seasonal_periods'] = st.number_input(
                        "Seasonal Periods",
                        min_value=2,
                        max_value=52,
                        value=12,
                        help="Number of periods in a season"
                    )
        
        st.markdown("---")
        
        # Feature Engineering Pipeline
        st.subheader("üîß Feature Engineering Pipeline")
        
        feature_config = {}
        
        col1, col2 = st.columns(2)
        
        with col1:
            feature_config['use_technical_indicators'] = st.checkbox(
                "Use Technical Indicators",
                value=True,
                help="Add technical indicators as features"
            )
            
            if feature_config['use_technical_indicators']:
                indicators = st.multiselect(
                    "Select Indicators",
                    ["SMA", "EMA", "RSI", "MACD", "Bollinger Bands", "Volume Indicators"],
                    default=["SMA", "RSI"],
                    help="Technical indicators to calculate"
                )
                feature_config['indicators'] = indicators
                
                if "SMA" in indicators or "EMA" in indicators:
                    ma_windows = st.multiselect(
                        "Moving Average Windows",
                        [5, 10, 20, 50, 100, 200],
                        default=[20, 50],
                        help="Periods for moving averages"
                    )
                    feature_config['ma_windows'] = ma_windows
            
            feature_config['use_lag_features'] = st.checkbox(
                "Use Lag Features",
                value=True,
                help="Add lagged values as features"
            )
            
            if feature_config['use_lag_features']:
                lag_periods = st.multiselect(
                    "Lag Periods",
                    [1, 2, 3, 5, 10, 20],
                    default=[1, 2, 3, 5],
                    help="Number of periods to lag"
                )
                feature_config['lag_periods'] = lag_periods
        
        with col2:
            feature_config['use_rolling_features'] = st.checkbox(
                "Use Rolling Features",
                value=False,
                help="Add rolling statistics as features"
            )
            
            if feature_config['use_rolling_features']:
                rolling_windows = st.multiselect(
                    "Rolling Windows",
                    [5, 10, 20, 30],
                    default=[5, 10],
                    help="Window sizes for rolling statistics"
                )
                feature_config['rolling_windows'] = rolling_windows
            
            feature_config['use_time_features'] = st.checkbox(
                "Use Time Features",
                value=False,
                help="Add time-based features (day of week, month, etc.)"
            )
            
            feature_config['normalize_features'] = st.checkbox(
                "Normalize Features",
                value=True,
                help="Normalize feature values"
            )
            
            if feature_config['normalize_features']:
                normalization_method = st.selectbox(
                    "Normalization Method",
                    ["StandardScaler", "MinMaxScaler", "RobustScaler"],
                    help="Normalization method"
                )
                feature_config['normalization_method'] = normalization_method
        
        st.markdown("---")
        
        # Data Splitting Configuration
        st.subheader("üìä Data Splitting & Validation")
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            train_split = st.slider(
                "Training Set (%)",
                min_value=60,
                max_value=90,
                value=70,
                step=5,
                help="Percentage for training"
            )
        
        with col2:
            val_split = st.slider(
                "Validation Set (%)",
                min_value=10,
                max_value=30,
                value=15,
                step=5,
                help="Percentage for validation"
            )
        
        with col3:
            test_split = 100 - train_split - val_split
            st.metric("Test Set (%)", f"{test_split}%")
        
        use_cross_validation = st.checkbox(
            "Use Cross-Validation",
            value=False,
            help="Enable k-fold cross-validation"
        )
        
        if use_cross_validation:
            cv_folds = st.number_input(
                "Number of Folds",
                min_value=3,
                max_value=10,
                value=5,
                help="Number of cross-validation folds"
            )
            feature_config['cv_folds'] = cv_folds
        
        st.markdown("---")
        
        # Training Configuration
        st.subheader("üéØ Training Configuration")
        
        target_column = st.selectbox(
            "Target Column",
            options=[col for col in data.columns if col.lower() in ['close', 'price', 'adj close']] or ['Close'],
            help="Column to use as target"
        )
        
        model_name = st.text_input(
            "Model Name",
            value=f"{model_type}_config_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            help="Name for saving the model"
        )
        
        # Train Button
        train_button = st.button("üöÄ Train Model with Configuration", type="primary", use_container_width=True)
        
        if train_button:
            try:
                progress_bar = st.progress(0)
                status_text = st.empty()
                
                # Feature Engineering
                status_text.text("Engineering features...")
                progress_bar.progress(0.1)
                
                try:
                    from trading.data.preprocessing import FeatureEngineering, DataPreprocessor
                    fe = FeatureEngineering()
                    processed_data = data.copy()
                    
                    if feature_config.get('use_technical_indicators'):
                        if 'SMA' in feature_config.get('indicators', []):
                            for window in feature_config.get('ma_windows', [20]):
                                processed_data[f'SMA_{window}'] = processed_data[target_column].rolling(window).mean()
                        
                        if 'RSI' in feature_config.get('indicators', []):
                            delta = processed_data[target_column].diff()
                            gain = (delta.where(delta > 0, 0)).rolling(14).mean()
                            loss = (-delta.where(delta < 0, 0)).rolling(14).mean()
                            rs = gain / loss
                            processed_data['RSI'] = 100 - (100 / (1 + rs))
                        
                        if 'MACD' in feature_config.get('indicators', []):
                            macd_features = fe.calculate_macd(processed_data)
                            processed_data = pd.concat([processed_data, macd_features], axis=1)
                    
                    if feature_config.get('use_lag_features'):
                        for lag in feature_config.get('lag_periods', [1, 2, 3]):
                            processed_data[f'lag_{lag}'] = processed_data[target_column].shift(lag)
                    
                    # Remove NaN
                    processed_data = processed_data.dropna()
                    
                    if feature_config.get('normalize_features'):
                        preprocessor = DataPreprocessor()
                        if feature_config.get('normalization_method') == 'StandardScaler':
                            processed_data = preprocessor.standardize(processed_data)
                        elif feature_config.get('normalization_method') == 'MinMaxScaler':
                            processed_data = preprocessor.normalize(processed_data)
                    
                except Exception as e:
                    st.warning(f"Feature engineering warning: {str(e)}")
                    processed_data = data.copy()
                
                # Split data
                status_text.text("Splitting data...")
                progress_bar.progress(0.3)
                
                target_series = processed_data[target_column].dropna()
                n = len(target_series)
                
                train_end = int(n * train_split / 100)
                val_end = int(n * (train_split + val_split) / 100)
                
                train_data = target_series[:train_end]
                val_data = target_series[train_end:val_end]
                test_data = target_series[val_end:]
                
                # Initialize and train model
                status_text.text(f"Initializing {model_type} model...")
                progress_bar.progress(0.4)
                
                model = None
                
                if model_type == "LSTM":
                    model = LSTMModel(model_config)
                elif model_type == "XGBoost":
                    model = XGBoostModel(model_config)
                elif model_type == "Prophet":
                    model = ProphetModel(model_config)
                elif model_type == "ARIMA":
                    model = ARIMAModel(model_config)
                
                if model is None:
                    st.error(f"Failed to initialize {model_type} model")
                else:
                    # Train model
                    status_text.text(f"Training {model_type} model...")
                    progress_bar.progress(0.5)
                    
                    try:
                        if model_type == "Prophet":
                            train_df = pd.DataFrame({
                                'ds': train_data.index,
                                'y': train_data.values
                            })
                            model.fit(train_df)
                        elif model_type == "ARIMA":
                            model.fit(train_data.values)
                        else:
                            X_train = train_data.values.reshape(-1, 1)
                            y_train = train_data.values
                            model.fit(X_train, y_train)
                        
                        progress_bar.progress(0.8)
                        status_text.text("Evaluating model...")
                        
                        # Evaluate
                        if model_type == "Prophet":
                            test_df = pd.DataFrame({
                                'ds': test_data.index,
                                'y': test_data.values
                            })
                            predictions = model.predict(test_df)
                            y_pred = predictions['yhat'].values if 'yhat' in predictions.columns else predictions.values.flatten()
                        elif model_type == "ARIMA":
                            predictions = model.predict(len(test_data))
                            y_pred = predictions
                        else:
                            X_test = test_data.values.reshape(-1, 1)
                            y_pred = model.predict(X_test)
                        
                        y_true = test_data.values
                        min_len = min(len(y_true), len(y_pred))
                        y_true = y_true[:min_len]
                        y_pred = y_pred[:min_len]
                        
                        # Calculate metrics
                        mse = np.mean((y_true - y_pred) ** 2)
                        mae = np.mean(np.abs(y_true - y_pred))
                        rmse = np.sqrt(mse)
                        mape = np.mean(np.abs((y_true - y_pred) / (y_true + 1e-8))) * 100
                        ss_res = np.sum((y_true - y_pred) ** 2)
                        ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)
                        r2 = 1 - (ss_res / (ss_tot + 1e-8))
                        
                        progress_bar.progress(1.0)
                        status_text.text("‚úÖ Training complete!")
                        
                        # Store results
                        training_result = {
                            "model": model,
                            "model_type": model_type,
                            "model_config": model_config,
                            "feature_config": feature_config,
                            "model_name": model_name,
                            "metrics": {
                                "MSE": mse,
                                "MAE": mae,
                                "RMSE": rmse,
                                "MAPE": mape,
                                "R¬≤": r2
                            },
                            "train_size": len(train_data),
                            "val_size": len(val_data),
                            "test_size": len(test_data),
                            "trained_at": datetime.now().isoformat()
                        }
                        
                        if 'configured_models' not in st.session_state:
                            st.session_state.configured_models = {}
                        st.session_state.configured_models[model_name] = training_result
                        
                        # Display results
                        st.success("‚úÖ Model trained successfully!")
                        
                        st.markdown("---")
                        st.subheader("üìä Training Results")
                        
                        # Metrics
                        col1, col2, col3, col4, col5 = st.columns(5)
                        with col1:
                            st.metric("RMSE", f"{rmse:.4f}")
                        with col2:
                            st.metric("MAE", f"{mae:.4f}")
                        with col3:
                            st.metric("MAPE", f"{mape:.2f}%")
                        with col4:
                            st.metric("R¬≤ Score", f"{r2:.4f}")
                        with col5:
                            st.metric("MSE", f"{mse:.4f}")
                        
                        # Prediction plot
                        fig = go.Figure()
                        fig.add_trace(go.Scatter(
                            x=list(range(len(y_true))),
                            y=y_true,
                            mode='lines',
                            name='Actual',
                            line=dict(color='blue', width=2)
                        ))
                        fig.add_trace(go.Scatter(
                            x=list(range(len(y_pred))),
                            y=y_pred,
                            mode='lines',
                            name='Predicted',
                            line=dict(color='red', width=2, dash='dash')
                        ))
                        fig.update_layout(
                            title="Prediction vs Actual (Test Set)",
                            xaxis_title="Time Step",
                            yaxis_title="Value",
                            hovermode='x unified',
                            height=400
                        )
                        st.plotly_chart(fig, use_container_width=True)
                        
                        # Save button
                        if st.button("üíæ Save Model", use_container_width=True):
                            try:
                                if st.session_state.model_registry:
                                    st.session_state.model_registry.save_model(
                                        model_name=model_name,
                                        model=model,
                                        metadata=training_result
                                    )
                                    st.success(f"‚úÖ Model '{model_name}' saved!")
                                else:
                                    if 'saved_models' not in st.session_state:
                                        st.session_state.saved_models = {}
                                    st.session_state.saved_models[model_name] = training_result
                                    st.success(f"‚úÖ Model '{model_name}' saved to session!")
                            except Exception as e:
                                st.error(f"Error saving model: {str(e)}")
                    
                    except Exception as e:
                        st.error(f"‚ùå Training failed: {str(e)}")
                        logger.exception("Training error")
                
                progress_bar.empty()
                status_text.empty()
            
            except Exception as e:
                st.error(f"‚ùå Error: {str(e)}")
                logger.exception("Configuration training error")
    
    else:
        st.info("üëÜ Please load data to begin configuration.")

# TAB 3: Hyperparameter Optimization
with tab3:
    st.header("üéØ Hyperparameter Optimization")
    st.markdown("Automated hyperparameter tuning using multiple optimization algorithms.")
    
    # Data Selection
    if st.session_state.training_data is not None:
        data = st.session_state.training_data
        st.info(f"‚úÖ Using data from previous tabs: {len(data)} records")
    else:
        st.warning("‚ö†Ô∏è Please load data in Tab 1 or Tab 2 first.")
        data = None
    
    if data is not None:
        st.markdown("---")
        
        # Model Selection
        st.subheader("ü§ñ Model Selection")
        model_type = st.selectbox(
            "Model Type",
            ["LSTM", "XGBoost", "Prophet", "ARIMA"],
            help="Select the model type to optimize"
        )
        
        target_column = st.selectbox(
            "Target Column",
            options=[col for col in data.columns if col.lower() in ['close', 'price', 'adj close']] or ['Close'],
            help="Column to use as target"
        )
        
        st.markdown("---")
        
        # Optimization Method Selection
        st.subheader("üîß Optimization Method")
        
        optimization_method = st.selectbox(
            "Optimization Algorithm",
            ["Grid Search", "Random Search", "Bayesian Optimization (Optuna)", "Genetic Algorithm"],
            help="Select the optimization algorithm"
        )
        
        # Optimization Configuration
        col1, col2 = st.columns(2)
        
        with col1:
            optimization_objective = st.selectbox(
                "Optimization Objective",
                ["Minimize RMSE", "Minimize MAE", "Minimize MSE", "Maximize R¬≤", "Minimize MAPE"],
                help="What metric to optimize"
            )
            
            n_trials = st.number_input(
                "Number of Trials",
                min_value=10,
                max_value=500,
                value=50,
                step=10,
                help="Number of optimization trials to run"
            )
        
        with col2:
            timeout_minutes = st.number_input(
                "Timeout (minutes)",
                min_value=5,
                max_value=120,
                value=30,
                step=5,
                help="Maximum time for optimization"
            )
            
            early_stopping = st.checkbox(
                "Enable Early Stopping",
                value=True,
                help="Stop if no improvement for N trials"
            )
            
            if early_stopping:
                early_stopping_patience = st.number_input(
                    "Early Stopping Patience",
                    min_value=5,
                    max_value=50,
                    value=10,
                    step=5,
                    help="Number of trials without improvement before stopping"
                )
        
        st.markdown("---")
        
        # Search Space Configuration
        st.subheader("üìä Search Space Configuration")
        st.markdown("Define the parameter ranges to search:")
        
        param_space = {}
        
        if model_type == "LSTM":
            with st.expander("LSTM Parameters", expanded=True):
                col1, col2 = st.columns(2)
                
                with col1:
                    hidden_dim_min = st.number_input("Hidden Dim (Min)", min_value=16, max_value=256, value=32, step=16)
                    hidden_dim_max = st.number_input("Hidden Dim (Max)", min_value=16, max_value=512, value=128, step=16)
                    param_space['hidden_dim'] = (hidden_dim_min, hidden_dim_max)
                    
                    num_layers_options = st.multiselect(
                        "Number of Layers",
                        [1, 2, 3, 4, 5],
                        default=[2, 3],
                        help="Number of LSTM layers to try"
                    )
                    param_space['num_layers'] = num_layers_options if num_layers_options else [2]
                
                with col2:
                    dropout_min = st.slider("Dropout (Min)", 0.0, 0.5, 0.1, 0.05)
                    dropout_max = st.slider("Dropout (Max)", 0.0, 0.5, 0.3, 0.05)
                    param_space['dropout'] = (dropout_min, dropout_max)
                    
                    learning_rate_options = st.multiselect(
                        "Learning Rate",
                        [0.0001, 0.0005, 0.001, 0.005, 0.01],
                        default=[0.001, 0.005],
                        help="Learning rates to try"
                    )
                    param_space['learning_rate'] = learning_rate_options if learning_rate_options else [0.001]
        
        elif model_type == "XGBoost":
            with st.expander("XGBoost Parameters", expanded=True):
                col1, col2 = st.columns(2)
                
                with col1:
                    n_estimators_min = st.number_input("N Estimators (Min)", min_value=50, max_value=500, value=100, step=50)
                    n_estimators_max = st.number_input("N Estimators (Max)", min_value=100, max_value=1000, value=300, step=50)
                    param_space['n_estimators'] = (n_estimators_min, n_estimators_max)
                    
                    max_depth_options = st.multiselect(
                        "Max Depth",
                        [3, 4, 5, 6, 7, 8, 9, 10],
                        default=[5, 6, 7],
                        help="Maximum tree depth"
                    )
                    param_space['max_depth'] = max_depth_options if max_depth_options else [6]
                
                with col2:
                    learning_rate_min = st.slider("Learning Rate (Min)", 0.01, 0.3, 0.05, 0.01)
                    learning_rate_max = st.slider("Learning Rate (Max)", 0.05, 0.3, 0.2, 0.01)
                    param_space['learning_rate'] = (learning_rate_min, learning_rate_max)
                    
                    subsample_min = st.slider("Subsample (Min)", 0.5, 1.0, 0.7, 0.1)
                    subsample_max = st.slider("Subsample (Max)", 0.7, 1.0, 1.0, 0.1)
                    param_space['subsample'] = (subsample_min, subsample_max)
        
        elif model_type == "Prophet":
            with st.expander("Prophet Parameters", expanded=True):
                col1, col2 = st.columns(2)
                
                with col1:
                    changepoint_min = st.number_input("Changepoint Prior Scale (Min)", 0.001, 0.5, 0.01, 0.01, format="%.3f")
                    changepoint_max = st.number_input("Changepoint Prior Scale (Max)", 0.01, 0.5, 0.1, 0.01, format="%.3f")
                    param_space['changepoint_prior_scale'] = (changepoint_min, changepoint_max)
                    
                    seasonality_min = st.number_input("Seasonality Prior Scale (Min)", 0.01, 50.0, 5.0, 1.0)
                    seasonality_max = st.number_input("Seasonality Prior Scale (Max)", 5.0, 50.0, 20.0, 1.0)
                    param_space['seasonality_prior_scale'] = (seasonality_min, seasonality_max)
                
                with col2:
                    seasonality_mode_options = st.multiselect(
                        "Seasonality Mode",
                        ["additive", "multiplicative"],
                        default=["additive"],
                        help="Seasonality mode"
                    )
                    param_space['seasonality_mode'] = seasonality_mode_options if seasonality_mode_options else ["additive"]
        
        elif model_type == "ARIMA":
            with st.expander("ARIMA Parameters", expanded=True):
                col1, col2 = st.columns(2)
                
                with col1:
                    p_min = st.number_input("AR Order p (Min)", 0, 10, 1, 1)
                    p_max = st.number_input("AR Order p (Max)", 1, 10, 5, 1)
                    param_space['p'] = (p_min, p_max)
                    
                    d_options = st.multiselect(
                        "I Order d",
                        [0, 1, 2, 3],
                        default=[0, 1],
                        help="Differencing order"
                    )
                    param_space['d'] = d_options if d_options else [1]
                
                with col2:
                    q_min = st.number_input("MA Order q (Min)", 0, 10, 0, 1)
                    q_max = st.number_input("MA Order q (Max)", 0, 10, 3, 1)
                    param_space['q'] = (q_min, q_max)
        
        st.markdown("---")
        
        # Run Optimization
        optimize_button = st.button("üöÄ Start Optimization", type="primary", use_container_width=True)
        
        if optimize_button:
            try:
                # Initialize progress tracking
                progress_container = st.container()
                results_container = st.container()
                
                with progress_container:
                    st.subheader("üîÑ Optimization Progress")
                    progress_bar = st.progress(0)
                    status_text = st.empty()
                    best_params_text = st.empty()
                    history_plot = st.empty()
                
                # Prepare data
                target_series = data[target_column].dropna()
                split_idx = int(len(target_series) * 0.8)
                train_data = target_series[:split_idx]
                test_data = target_series[split_idx:]
                
                # Define objective function
                def objective_function(params):
                    try:
                        # Create model with params
                        if model_type == "LSTM":
                            config = {
                                "input_dim": 1,
                                "hidden_dim": params.get('hidden_dim', 64),
                                "num_layers": params.get('num_layers', 2),
                                "dropout": params.get('dropout', 0.2),
                                "learning_rate": params.get('learning_rate', 0.001),
                                "output_dim": 1
                            }
                            model = LSTMModel(config)
                            X_train = train_data.values.reshape(-1, 1)
                            y_train = train_data.values
                            model.fit(X_train, y_train)
                            X_test = test_data.values.reshape(-1, 1)
                            y_pred = model.predict(X_test)
                        
                        elif model_type == "XGBoost":
                            config = {
                                "n_estimators": params.get('n_estimators', 100),
                                "max_depth": params.get('max_depth', 6),
                                "learning_rate": params.get('learning_rate', 0.1),
                                "subsample": params.get('subsample', 0.8)
                            }
                            model = XGBoostModel(config)
                            X_train = train_data.values.reshape(-1, 1)
                            y_train = train_data.values
                            model.fit(X_train, y_train)
                            X_test = test_data.values.reshape(-1, 1)
                            y_pred = model.predict(X_test)
                        
                        elif model_type == "Prophet":
                            config = {
                                "changepoint_prior_scale": params.get('changepoint_prior_scale', 0.05),
                                "seasonality_prior_scale": params.get('seasonality_prior_scale', 10.0),
                                "seasonality_mode": params.get('seasonality_mode', 'additive')
                            }
                            model = ProphetModel(config)
                            train_df = pd.DataFrame({
                                'ds': train_data.index,
                                'y': train_data.values
                            })
                            model.fit(train_df)
                            test_df = pd.DataFrame({
                                'ds': test_data.index,
                                'y': test_data.values
                            })
                            predictions = model.predict(test_df)
                            y_pred = predictions['yhat'].values if 'yhat' in predictions.columns else predictions.values.flatten()
                        
                        elif model_type == "ARIMA":
                            config = {
                                "order": (params.get('p', 5), params.get('d', 1), params.get('q', 0)),
                                "use_auto_arima": False
                            }
                            model = ARIMAModel(config)
                            model.fit(train_data.values)
                            y_pred = model.predict(len(test_data))
                        
                        # Calculate metric
                        y_true = test_data.values
                        min_len = min(len(y_true), len(y_pred))
                        y_true = y_true[:min_len]
                        y_pred = y_pred[:min_len]
                        
                        if "RMSE" in optimization_objective:
                            metric = np.sqrt(np.mean((y_true - y_pred) ** 2))
                        elif "MAE" in optimization_objective:
                            metric = np.mean(np.abs(y_true - y_pred))
                        elif "MSE" in optimization_objective:
                            metric = np.mean((y_true - y_pred) ** 2)
                        elif "MAPE" in optimization_objective:
                            metric = np.mean(np.abs((y_true - y_pred) / (y_true + 1e-8))) * 100
                        else:  # R¬≤
                            ss_res = np.sum((y_true - y_pred) ** 2)
                            ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)
                            metric = 1 - (ss_res / (ss_tot + 1e-8))
                            metric = -metric  # Negate for minimization
                        
                        return metric
                    
                    except Exception as e:
                        logger.warning(f"Objective function error: {e}")
                        return float('inf')
                
                # Run optimization based on method
                all_trials = []
                best_score = float('inf') if "Minimize" in optimization_objective else float('-inf')
                best_params = None
                convergence_history = []
                
                start_time = datetime.now()
                
                if optimization_method == "Grid Search":
                    # Generate all combinations
                    import itertools
                    param_names = list(param_space.keys())
                    param_values = []
                    for name in param_names:
                        space = param_space[name]
                        if isinstance(space, tuple):
                            # Range - generate values
                            if isinstance(space[0], int):
                                values = list(range(space[0], space[1] + 1, max(1, (space[1] - space[0]) // 5)))
                            else:
                                values = np.linspace(space[0], space[1], 5).tolist()
                        else:
                            values = space if isinstance(space, list) else [space]
                        param_values.append(values)
                    
                    combinations = list(itertools.product(*param_values))
                    total_combinations = min(len(combinations), n_trials)
                    
                    for i, combo in enumerate(combinations[:total_combinations]):
                        params = dict(zip(param_names, combo))
                        score = objective_function(params)
                        
                        all_trials.append({
                            "trial": i + 1,
                            "params": params,
                            "score": score
                        })
                        
                        if ("Minimize" in optimization_objective and score < best_score) or \
                           ("Maximize" in optimization_objective and score > best_score):
                            best_score = score
                            best_params = params.copy()
                        
                        convergence_history.append(best_score)
                        
                        # Update progress
                        progress = (i + 1) / total_combinations
                        progress_bar.progress(progress)
                        status_text.text(f"Trial {i+1}/{total_combinations} | Best Score: {best_score:.4f}")
                        best_params_text.json(best_params)
                        
                        # Update history plot
                        fig = go.Figure()
                        fig.add_trace(go.Scatter(
                            y=convergence_history,
                            mode='lines+markers',
                            name='Best Score',
                            line=dict(color='blue', width=2)
                        ))
                        fig.update_layout(
                            title="Optimization History",
                            xaxis_title="Trial",
                            yaxis_title="Best Score",
                            height=300
                        )
                        history_plot.plotly_chart(fig, use_container_width=True)
                
                elif optimization_method == "Random Search":
                    for i in range(n_trials):
                        # Sample random parameters
                        params = {}
                        for name, space in param_space.items():
                            if isinstance(space, tuple):
                                if isinstance(space[0], int):
                                    params[name] = np.random.randint(space[0], space[1] + 1)
                                else:
                                    params[name] = np.random.uniform(space[0], space[1])
                            elif isinstance(space, list):
                                params[name] = np.random.choice(space)
                            else:
                                params[name] = space
                        
                        score = objective_function(params)
                        
                        all_trials.append({
                            "trial": i + 1,
                            "params": params,
                            "score": score
                        })
                        
                        if ("Minimize" in optimization_objective and score < best_score) or \
                           ("Maximize" in optimization_objective and score > best_score):
                            best_score = score
                            best_params = params.copy()
                        
                        convergence_history.append(best_score)
                        
                        # Update progress
                        progress = (i + 1) / n_trials
                        progress_bar.progress(progress)
                        status_text.text(f"Trial {i+1}/{n_trials} | Best Score: {best_score:.4f}")
                        best_params_text.json(best_params)
                        
                        # Update history plot
                        fig = go.Figure()
                        fig.add_trace(go.Scatter(
                            y=convergence_history,
                            mode='lines+markers',
                            name='Best Score',
                            line=dict(color='blue', width=2)
                        ))
                        fig.update_layout(
                            title="Optimization History",
                            xaxis_title="Trial",
                            yaxis_title="Best Score",
                            height=300
                        )
                        history_plot.plotly_chart(fig, use_container_width=True)
                
                elif optimization_method == "Bayesian Optimization (Optuna)":
                    try:
                        import optuna
                        
                        def optuna_objective(trial):
                            # Suggest parameters
                            params = {}
                            for name, space in param_space.items():
                                if isinstance(space, tuple):
                                    if isinstance(space[0], int):
                                        params[name] = trial.suggest_int(name, space[0], space[1])
                                    else:
                                        params[name] = trial.suggest_float(name, space[0], space[1])
                                elif isinstance(space, list):
                                    params[name] = trial.suggest_categorical(name, space)
                                else:
                                    params[name] = space
                            
                            return objective_function(params)
                        
                        study = optuna.create_study(
                            direction="minimize" if "Minimize" in optimization_objective else "maximize"
                        )
                        
                        study.optimize(optuna_objective, n_trials=n_trials, show_progress_bar=False)
                        
                        best_params = study.best_params
                        best_score = study.best_value
                        
                        # Extract all trials
                        for i, trial in enumerate(study.trials):
                            all_trials.append({
                                "trial": i + 1,
                                "params": trial.params,
                                "score": trial.value
                            })
                            convergence_history.append(study.best_value if i == 0 else min(convergence_history[-1], trial.value) if "Minimize" in optimization_objective else max(convergence_history[-1], trial.value))
                        
                        progress_bar.progress(1.0)
                        status_text.text(f"‚úÖ Optimization complete! Best Score: {best_score:.4f}")
                    
                    except ImportError:
                        st.error("Optuna not available. Please install: pip install optuna")
                        return
                
                elif optimization_method == "Genetic Algorithm":
                    st.info("Genetic Algorithm optimization is a placeholder. Using Random Search instead.")
                    # Placeholder - would need DEAP or similar library
                    for i in range(n_trials):
                        params = {}
                        for name, space in param_space.items():
                            if isinstance(space, tuple):
                                if isinstance(space[0], int):
                                    params[name] = np.random.randint(space[0], space[1] + 1)
                                else:
                                    params[name] = np.random.uniform(space[0], space[1])
                            elif isinstance(space, list):
                                params[name] = np.random.choice(space)
                            else:
                                params[name] = space
                        
                        score = objective_function(params)
                        all_trials.append({
                            "trial": i + 1,
                            "params": params,
                            "score": score
                        })
                        
                        if ("Minimize" in optimization_objective and score < best_score) or \
                           ("Maximize" in optimization_objective and score > best_score):
                            best_score = score
                            best_params = params.copy()
                        
                        convergence_history.append(best_score)
                        progress = (i + 1) / n_trials
                        progress_bar.progress(progress)
                        status_text.text(f"Trial {i+1}/{n_trials} | Best Score: {best_score:.4f}")
                
                optimization_time = (datetime.now() - start_time).total_seconds()
                
                # Display Results
                with results_container:
                    st.markdown("---")
                    st.subheader("üìä Optimization Results")
                    
                    col1, col2, col3 = st.columns(3)
                    with col1:
                        st.metric("Best Score", f"{best_score:.4f}")
                    with col2:
                        st.metric("Total Trials", len(all_trials))
                    with col3:
                        st.metric("Optimization Time", f"{optimization_time:.1f}s")
                    
                    st.markdown("**Best Parameters:**")
                    st.json(best_params)
                    
                    # Results Table
                    st.markdown("**All Trials:**")
                    results_df = pd.DataFrame([
                        {
                            "Trial": t["trial"],
                            "Score": f"{t['score']:.4f}",
                            **{k: str(v) for k, v in t["params"].items()}
                        }
                        for t in all_trials
                    ])
                    st.dataframe(results_df, use_container_width=True, height=400)
                    
                    # Export button
                    csv = results_df.to_csv(index=False)
                    st.download_button(
                        label="üì• Download Results (CSV)",
                        data=csv,
                        file_name=f"optimization_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                        mime="text/csv"
                    )
                    
                    # Save best parameters
                    if st.button("üíæ Save Best Parameters", use_container_width=True):
                        if 'optimization_results' not in st.session_state:
                            st.session_state.optimization_results = {}
                        st.session_state.optimization_results[f"{model_type}_{optimization_method}"] = {
                            "best_params": best_params,
                            "best_score": best_score,
                            "all_trials": all_trials,
                            "optimization_time": optimization_time,
                            "optimized_at": datetime.now().isoformat()
                        }
                        st.success("‚úÖ Best parameters saved!")
            
            except Exception as e:
                st.error(f"‚ùå Optimization failed: {str(e)}")
                logger.exception("Optimization error")

# TAB 4: Model Performance
with tab4:
    st.header("üìä Model Performance")
    st.markdown("Comprehensive performance tracking and evaluation for trained models.")
    
    # Model Selection
    st.subheader("üîç Select Model")
    
    # Get available models from session state
    available_models = {}
    
    if 'quick_training_results' in st.session_state:
        for name, result in st.session_state.quick_training_results.items():
            available_models[name] = result
    
    if 'configured_models' in st.session_state:
        for name, result in st.session_state.configured_models.items():
            available_models[name] = result
    
    if 'saved_models' in st.session_state:
        for name, result in st.session_state.saved_models.items():
            available_models[name] = result
    
    if not available_models:
        st.warning("‚ö†Ô∏è No trained models found. Please train a model in Tab 1 or Tab 2 first.")
    else:
        selected_model_name = st.selectbox(
            "Select Model",
            options=list(available_models.keys()),
            help="Choose a model to view performance metrics"
        )
        
        if selected_model_name:
            model_result = available_models[selected_model_name]
            
            st.markdown("---")
            
            # Model Info
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("Model Type", model_result.get('model_type', 'Unknown'))
            with col2:
                st.metric("Train Size", model_result.get('train_size', 'N/A'))
            with col3:
                st.metric("Test Size", model_result.get('test_size', 'N/A'))
            
            st.markdown("---")
            
            # Performance Metrics Tabs
            perf_tab1, perf_tab2, perf_tab3, perf_tab4 = st.tabs([
                "üìà Training Metrics",
                "‚úÖ Validation Metrics",
                "üß™ Test Set Evaluation",
                "üìâ Performance Over Time"
            ])
            
            with perf_tab1:
                st.subheader("üìà Training Metrics")
                
                # Get metrics
                metrics = model_result.get('metrics', {})
                
                if metrics:
                    # Metrics Cards
                    col1, col2, col3, col4, col5 = st.columns(5)
                    with col1:
                        st.metric("RMSE", f"{metrics.get('RMSE', 0):.4f}")
                    with col2:
                        st.metric("MAE", f"{metrics.get('MAE', 0):.4f}")
                    with col3:
                        st.metric("MAPE", f"{metrics.get('MAPE', 0):.2f}%")
                    with col4:
                        st.metric("R¬≤ Score", f"{metrics.get('R¬≤', 0):.4f}")
                    with col5:
                        st.metric("MSE", f"{metrics.get('MSE', 0):.4f}")
                    
                    st.markdown("---")
                    
                    # Loss Curves (if available)
                    if 'training_history' in model_result:
                        st.markdown("**Training History**")
                        history = model_result['training_history']
                        
                        if 'train_loss' in history and 'val_loss' in history:
                            fig = go.Figure()
                            fig.add_trace(go.Scatter(
                                y=history['train_loss'],
                                mode='lines',
                                name='Train Loss',
                                line=dict(color='blue', width=2)
                            ))
                            fig.add_trace(go.Scatter(
                                y=history['val_loss'],
                                mode='lines',
                                name='Validation Loss',
                                line=dict(color='red', width=2)
                            ))
                            fig.update_layout(
                                title="Training and Validation Loss",
                                xaxis_title="Epoch",
                                yaxis_title="Loss",
                                hovermode='x unified',
                                height=400
                            )
                            st.plotly_chart(fig, use_container_width=True)
                        
                        if 'train_accuracy' in history and 'val_accuracy' in history:
                            fig = go.Figure()
                            fig.add_trace(go.Scatter(
                                y=history['train_accuracy'],
                                mode='lines',
                                name='Train Accuracy',
                                line=dict(color='green', width=2)
                            ))
                            fig.add_trace(go.Scatter(
                                y=history['val_accuracy'],
                                mode='lines',
                                name='Validation Accuracy',
                                line=dict(color='orange', width=2)
                            ))
                            fig.update_layout(
                                title="Training and Validation Accuracy",
                                xaxis_title="Epoch",
                                yaxis_title="Accuracy",
                                hovermode='x unified',
                                height=400
                            )
                            st.plotly_chart(fig, use_container_width=True)
                    else:
                        st.info("Training history not available for this model.")
                    
                    # Epoch-by-epoch metrics table
                    if 'epoch_metrics' in model_result:
                        st.markdown("**Epoch-by-Epoch Metrics**")
                        epoch_df = pd.DataFrame(model_result['epoch_metrics'])
                        st.dataframe(epoch_df, use_container_width=True)
            
            with perf_tab2:
                st.subheader("‚úÖ Validation Metrics")
                
                # Prediction vs Actual
                if 'predictions' in model_result and 'actuals' in model_result:
                    y_pred = model_result['predictions']
                    y_true = model_result['actuals']
                    
                    # Scatter plot
                    fig = go.Figure()
                    fig.add_trace(go.Scatter(
                        x=y_true,
                        y=y_pred,
                        mode='markers',
                        name='Predictions',
                        marker=dict(color='blue', size=5, opacity=0.6)
                    ))
                    # Perfect prediction line
                    min_val = min(min(y_true), min(y_pred))
                    max_val = max(max(y_true), max(y_pred))
                    fig.add_trace(go.Scatter(
                        x=[min_val, max_val],
                        y=[min_val, max_val],
                        mode='lines',
                        name='Perfect Prediction',
                        line=dict(color='red', width=2, dash='dash')
                    ))
                    fig.update_layout(
                        title="Prediction vs Actual (Scatter Plot)",
                        xaxis_title="Actual Values",
                        yaxis_title="Predicted Values",
                        hovermode='closest',
                        height=500
                    )
                    st.plotly_chart(fig, use_container_width=True)
                    
                    # Residual Plot
                    residuals = np.array(y_true) - np.array(y_pred)
                    
                    fig = go.Figure()
                    fig.add_trace(go.Scatter(
                        x=y_pred,
                        y=residuals,
                        mode='markers',
                        name='Residuals',
                        marker=dict(color='blue', size=5, opacity=0.6)
                    ))
                    fig.add_hline(y=0, line_dash="dash", line_color="red", annotation_text="Zero Residual")
                    fig.update_layout(
                        title="Residual Plot",
                        xaxis_title="Predicted Values",
                        yaxis_title="Residuals (Actual - Predicted)",
                        hovermode='closest',
                        height=400
                    )
                    st.plotly_chart(fig, use_container_width=True)
                    
                    # Residual Statistics
                    col1, col2, col3 = st.columns(3)
                    with col1:
                        st.metric("Mean Residual", f"{np.mean(residuals):.4f}")
                    with col2:
                        st.metric("Std Residual", f"{np.std(residuals):.4f}")
                    with col3:
                        st.metric("Max Residual", f"{np.max(np.abs(residuals)):.4f}")
                    
                    # Residual Distribution
                    fig = go.Figure()
                    fig.add_trace(go.Histogram(
                        x=residuals,
                        nbinsx=30,
                        name='Residual Distribution',
                        marker_color='blue'
                    ))
                    fig.add_vline(x=0, line_dash="dash", line_color="red", annotation_text="Zero")
                    fig.update_layout(
                        title="Residual Distribution",
                        xaxis_title="Residuals",
                        yaxis_title="Frequency",
                        height=400
                    )
                    st.plotly_chart(fig, use_container_width=True)
                else:
                    st.info("Validation predictions not available. Please evaluate the model first.")
            
            with perf_tab3:
                st.subheader("üß™ Test Set Evaluation")
                
                metrics = model_result.get('metrics', {})
                
                if metrics:
                    # Test Metrics Summary
                    st.markdown("**Test Set Performance**")
                    
                    col1, col2, col3, col4, col5 = st.columns(5)
                    with col1:
                        st.metric("RMSE", f"{metrics.get('RMSE', 0):.4f}")
                    with col2:
                        st.metric("MAE", f"{metrics.get('MAE', 0):.4f}")
                    with col3:
                        st.metric("MAPE", f"{metrics.get('MAPE', 0):.2f}%")
                    with col4:
                        st.metric("R¬≤ Score", f"{metrics.get('R¬≤', 0):.4f}")
                    with col5:
                        st.metric("MSE", f"{metrics.get('MSE', 0):.4f}")
                    
                    st.markdown("---")
                    
                    # Error Analysis
                    st.markdown("**Error Analysis**")
                    
                    if 'predictions' in model_result and 'actuals' in model_result:
                        y_pred = np.array(model_result['predictions'])
                        y_true = np.array(model_result['actuals'])
                        errors = np.abs(y_true - y_pred)
                        
                        col1, col2 = st.columns(2)
                        
                        with col1:
                            # Error Distribution
                            fig = go.Figure()
                            fig.add_trace(go.Histogram(
                                x=errors,
                                nbinsx=30,
                                name='Error Distribution',
                                marker_color='red'
                            ))
                            fig.update_layout(
                                title="Absolute Error Distribution",
                                xaxis_title="Absolute Error",
                                yaxis_title="Frequency",
                                height=400
                            )
                            st.plotly_chart(fig, use_container_width=True)
                        
                        with col2:
                            # Error over Time
                            fig = go.Figure()
                            fig.add_trace(go.Scatter(
                                y=errors,
                                mode='lines+markers',
                                name='Absolute Error',
                                line=dict(color='red', width=2),
                                marker=dict(size=4)
                            ))
                            fig.update_layout(
                                title="Error Over Time",
                                xaxis_title="Time Step",
                                yaxis_title="Absolute Error",
                                hovermode='x unified',
                                height=400
                            )
                            st.plotly_chart(fig, use_container_width=True)
                        
                        # Error Statistics
                        col1, col2, col3, col4 = st.columns(4)
                        with col1:
                            st.metric("Mean Error", f"{np.mean(errors):.4f}")
                        with col2:
                            st.metric("Median Error", f"{np.median(errors):.4f}")
                        with col3:
                            st.metric("Max Error", f"{np.max(errors):.4f}")
                        with col4:
                            st.metric("95th Percentile", f"{np.percentile(errors, 95):.4f}")
                else:
                    st.info("Test metrics not available.")
            
            with perf_tab4:
                st.subheader("üìâ Performance Over Time")
                
                # Performance tracking over time
                if 'performance_history' in model_result:
                    perf_history = model_result['performance_history']
                    
                    # Performance trends
                    fig = go.Figure()
                    
                    if 'rmse_history' in perf_history:
                        fig.add_trace(go.Scatter(
                            y=perf_history['rmse_history'],
                            mode='lines+markers',
                            name='RMSE',
                            line=dict(color='red', width=2)
                        ))
                    
                    if 'mae_history' in perf_history:
                        fig.add_trace(go.Scatter(
                            y=perf_history['mae_history'],
                            mode='lines+markers',
                            name='MAE',
                            line=dict(color='orange', width=2)
                        ))
                    
                    if 'r2_history' in perf_history:
                        fig.add_trace(go.Scatter(
                            y=perf_history['r2_history'],
                            mode='lines+markers',
                            name='R¬≤',
                            line=dict(color='green', width=2)
                        ))
                    
                    fig.update_layout(
                        title="Performance Metrics Over Time",
                        xaxis_title="Evaluation Period",
                        yaxis_title="Metric Value",
                        hovermode='x unified',
                        height=400
                    )
                    st.plotly_chart(fig, use_container_width=True)
                else:
                    st.info("Performance history not available. This feature tracks model performance over multiple evaluations.")
                
                # Model Degradation Detection
                st.markdown("---")
                st.markdown("**Model Degradation Detection**")
                
                if 'performance_history' in model_result and 'rmse_history' in model_result['performance_history']:
                    rmse_history = model_result['performance_history']['rmse_history']
                    
                    if len(rmse_history) > 5:
                        # Calculate trend
                        recent_rmse = np.mean(rmse_history[-5:])
                        earlier_rmse = np.mean(rmse_history[:5])
                        degradation = ((recent_rmse - earlier_rmse) / earlier_rmse) * 100
                        
                        col1, col2 = st.columns(2)
                        
                        with col1:
                            st.metric("Early RMSE", f"{earlier_rmse:.4f}")
                            st.metric("Recent RMSE", f"{recent_rmse:.4f}")
                        
                        with col2:
                            if degradation > 5:
                                st.error(f"‚ö†Ô∏è Model Degradation Detected: {degradation:.2f}% increase in RMSE")
                            elif degradation < -5:
                                st.success(f"‚úÖ Model Improvement: {abs(degradation):.2f}% decrease in RMSE")
                            else:
                                st.info(f"‚ÑπÔ∏è Model Stable: {degradation:.2f}% change in RMSE")
                    else:
                        st.info("Insufficient history for degradation detection (need at least 5 evaluations).")
                else:
                    st.info("Performance history not available for degradation detection.")
            
            # Export Performance Report
            st.markdown("---")
            col1, col2 = st.columns(2)
            
            with col1:
                if st.button("üì• Export Performance Report", use_container_width=True):
                    # Create report
                    report = {
                        "model_name": selected_model_name,
                        "model_type": model_result.get('model_type'),
                        "metrics": model_result.get('metrics', {}),
                        "train_size": model_result.get('train_size'),
                        "test_size": model_result.get('test_size'),
                        "trained_at": model_result.get('trained_at'),
                        "evaluated_at": datetime.now().isoformat()
                    }
                    
                    import json
                    report_json = json.dumps(report, indent=2)
                    st.download_button(
                        label="Download JSON Report",
                        data=report_json,
                        file_name=f"performance_report_{selected_model_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
                        mime="application/json"
                    )
            
            with col2:
                if st.button("üîÑ Re-evaluate Model", use_container_width=True):
                    st.info("Re-evaluation functionality would re-run the model on test data.")

# TAB 5: Model Comparison
with tab5:
    st.header("üîç Model Comparison")
    st.markdown("Compare multiple trained models side-by-side and create ensemble forecasts.")
    
    # Get available models
    available_models = {}
    
    if 'quick_training_results' in st.session_state:
        for name, result in st.session_state.quick_training_results.items():
            available_models[name] = result
    
    if 'configured_models' in st.session_state:
        for name, result in st.session_state.configured_models.items():
            available_models[name] = result
    
    if 'saved_models' in st.session_state:
        for name, result in st.session_state.saved_models.items():
            available_models[name] = result
    
    if len(available_models) < 2:
        st.warning("‚ö†Ô∏è Please train at least 2 models in Tab 1 or Tab 2 to enable comparison.")
    else:
        st.subheader("üìä Select Models to Compare")
        
        model_names = list(available_models.keys())
        selected_models = st.multiselect(
            "Choose models to compare (select 2-5 models):",
            model_names,
            default=model_names[:min(3, len(model_names))] if len(model_names) >= 2 else [],
            help="Select 2-5 models for comparison"
        )
        
        if len(selected_models) < 2:
            st.warning("Please select at least 2 models for comparison.")
        elif len(selected_models) > 5:
            st.warning("Please select no more than 5 models for comparison.")
        else:
            st.markdown("---")
            
            # Comparison Metrics Table
            st.subheader("üìà Comparison Metrics")
            
            # Build comparison table
            comparison_data = []
            
            for model_name in selected_models:
                model_result = available_models[model_name]
                metrics = model_result.get('metrics', {})
                
                comparison_data.append({
                    "Model": model_name,
                    "Type": model_result.get('model_type', 'Unknown'),
                    "RMSE": f"{metrics.get('RMSE', 0):.4f}",
                    "MAE": f"{metrics.get('MAE', 0):.4f}",
                    "MAPE": f"{metrics.get('MAPE', 0):.2f}%",
                    "R¬≤": f"{metrics.get('R¬≤', 0):.4f}",
                    "MSE": f"{metrics.get('MSE', 0):.4f}",
                    "Train Size": model_result.get('train_size', 'N/A'),
                    "Test Size": model_result.get('test_size', 'N/A')
                })
            
            comparison_df = pd.DataFrame(comparison_data)
            st.dataframe(comparison_df, use_container_width=True)
            
            # Find best model for each metric
            st.markdown("**üèÜ Best Models by Metric:**")
            col1, col2, col3, col4, col5 = st.columns(5)
            
            with col1:
                best_rmse = comparison_df.loc[comparison_df['RMSE'].astype(float).idxmin(), 'Model']
                st.metric("Best RMSE", best_rmse)
            
            with col2:
                best_mae = comparison_df.loc[comparison_df['MAE'].astype(float).idxmin(), 'Model']
                st.metric("Best MAE", best_mae)
            
            with col3:
                best_mape = comparison_df.loc[comparison_df['MAPE'].str.replace('%', '').astype(float).idxmin(), 'Model']
                st.metric("Best MAPE", best_mape)
            
            with col4:
                best_r2 = comparison_df.loc[comparison_df['R¬≤'].astype(float).idxmax(), 'Model']
                st.metric("Best R¬≤", best_r2)
            
            with col5:
                best_mse = comparison_df.loc[comparison_df['MSE'].astype(float).idxmin(), 'Model']
                st.metric("Best MSE", best_mse)
            
            st.markdown("---")
            
            # Performance Charts Overlay
            st.subheader("üìä Performance Charts Overlay")
            
            # Metrics comparison chart
            chart_type = st.radio(
                "Chart Type",
                ["Bar Chart", "Line Chart"],
                horizontal=True
            )
            
            if chart_type == "Bar Chart":
                fig = go.Figure()
                
                models = comparison_df['Model'].tolist()
                rmse_values = comparison_df['RMSE'].astype(float).tolist()
                mae_values = comparison_df['MAE'].astype(float).tolist()
                r2_values = comparison_df['R¬≤'].astype(float).tolist()
                
                fig.add_trace(go.Bar(
                    name='RMSE',
                    x=models,
                    y=rmse_values,
                    marker_color='red'
                ))
                fig.add_trace(go.Bar(
                    name='MAE',
                    x=models,
                    y=mae_values,
                    marker_color='orange'
                ))
                fig.add_trace(go.Bar(
                    name='R¬≤',
                    x=models,
                    y=r2_values,
                    marker_color='green'
                ))
                
                fig.update_layout(
                    title="Model Comparison - Metrics Bar Chart",
                    xaxis_title="Model",
                    yaxis_title="Metric Value",
                    barmode='group',
                    height=500
                )
                st.plotly_chart(fig, use_container_width=True)
            else:
                # Line chart for metrics over models
                fig = go.Figure()
                
                models = comparison_df['Model'].tolist()
                rmse_values = comparison_df['RMSE'].astype(float).tolist()
                mae_values = comparison_df['MAE'].astype(float).tolist()
                r2_values = comparison_df['R¬≤'].astype(float).tolist()
                
                fig.add_trace(go.Scatter(
                    x=models,
                    y=rmse_values,
                    mode='lines+markers',
                    name='RMSE',
                    line=dict(color='red', width=2),
                    marker=dict(size=8)
                ))
                fig.add_trace(go.Scatter(
                    x=models,
                    y=mae_values,
                    mode='lines+markers',
                    name='MAE',
                    line=dict(color='orange', width=2),
                    marker=dict(size=8)
                ))
                fig.add_trace(go.Scatter(
                    x=models,
                    y=r2_values,
                    mode='lines+markers',
                    name='R¬≤',
                    line=dict(color='green', width=2),
                    marker=dict(size=8)
                ))
                
                fig.update_layout(
                    title="Model Comparison - Metrics Line Chart",
                    xaxis_title="Model",
                    yaxis_title="Metric Value",
                    hovermode='x unified',
                    height=500
                )
                st.plotly_chart(fig, use_container_width=True)
            
            # Prediction Overlay Chart
            st.markdown("**Prediction Overlay:**")
            
            if all('predictions' in available_models[m] and 'actuals' in available_models[m] for m in selected_models):
                fig = go.Figure()
                
                # Get actual values (use first model's actuals)
                first_model = selected_models[0]
                y_actual = available_models[first_model]['actuals']
                
                # Add actual values
                fig.add_trace(go.Scatter(
                    x=list(range(len(y_actual))),
                    y=y_actual,
                    mode='lines',
                    name='Actual',
                    line=dict(color='black', width=3)
                ))
                
                # Add predictions from each model
                colors = ['red', 'blue', 'green', 'orange', 'purple']
                for idx, model_name in enumerate(selected_models):
                    model_result = available_models[model_name]
                    y_pred = model_result['predictions']
                    
                    # Ensure same length
                    min_len = min(len(y_actual), len(y_pred))
                    y_pred_aligned = y_pred[:min_len]
                    
                    fig.add_trace(go.Scatter(
                        x=list(range(min_len)),
                        y=y_pred_aligned,
                        mode='lines',
                        name=f"{model_name} (Predicted)",
                        line=dict(color=colors[idx % len(colors)], width=2, dash='dash')
                    ))
                
                fig.update_layout(
                    title="Prediction Overlay - All Models",
                    xaxis_title="Time Step",
                    yaxis_title="Value",
                    hovermode='x unified',
                    height=500
                )
                st.plotly_chart(fig, use_container_width=True)
            else:
                st.info("Prediction data not available for all selected models.")
            
            st.markdown("---")
            
            # Statistical Significance Tests
            st.subheader("üìä Statistical Significance Tests")
            
            if len(selected_models) >= 2:
                # Perform pairwise comparisons
                from scipy import stats
                
                significance_results = []
                
                for i in range(len(selected_models)):
                    for j in range(i + 1, len(selected_models)):
                        model1 = selected_models[i]
                        model2 = selected_models[j]
                        
                        if 'predictions' in available_models[model1] and 'predictions' in available_models[model2]:
                            pred1 = np.array(available_models[model1]['predictions'])
                            pred2 = np.array(available_models[model2]['predictions'])
                            actual = np.array(available_models[model1].get('actuals', available_models[model2].get('actuals', [])))
                            
                            if len(actual) > 0:
                                # Calculate errors
                                errors1 = np.abs(actual[:min(len(actual), len(pred1))] - pred1[:min(len(actual), len(pred1))])
                                errors2 = np.abs(actual[:min(len(actual), len(pred2))] - pred2[:min(len(actual), len(pred2))])
                                
                                # Ensure same length
                                min_len = min(len(errors1), len(errors2))
                                errors1 = errors1[:min_len]
                                errors2 = errors2[:min_len]
                                
                                if len(errors1) > 10:  # Need sufficient data
                                    # Paired t-test
                                    try:
                                        t_stat, p_value = stats.ttest_rel(errors1, errors2)
                                        
                                        significance_results.append({
                                            "Model 1": model1,
                                            "Model 2": model2,
                                            "T-Statistic": f"{t_stat:.4f}",
                                            "P-Value": f"{p_value:.4f}",
                                            "Significant": "Yes" if p_value < 0.05 else "No",
                                            "Better Model": model1 if np.mean(errors1) < np.mean(errors2) else model2
                                        })
                                    except Exception as e:
                                        logger.warning(f"Statistical test failed: {e}")
                
                if significance_results:
                    significance_df = pd.DataFrame(significance_results)
                    st.dataframe(significance_df, use_container_width=True)
                    
                    st.caption("Note: P-value < 0.05 indicates statistically significant difference between models.")
                else:
                    st.info("Statistical significance tests require prediction data. Please ensure models have been evaluated.")
            
            st.markdown("---")
            
            # Model Selection Recommendation
            st.subheader("üéØ Model Selection Recommendation")
            
            # Calculate overall score (weighted average of normalized metrics)
            model_scores = {}
            
            for model_name in selected_models:
                model_result = available_models[model_name]
                metrics = model_result.get('metrics', {})
                
                # Normalize metrics (lower is better for RMSE, MAE, MSE, MAPE; higher is better for R¬≤)
                rmse = metrics.get('RMSE', float('inf'))
                mae = metrics.get('MAE', float('inf'))
                mape = metrics.get('MAPE', float('inf'))
                r2 = metrics.get('R¬≤', 0)
                mse = metrics.get('MSE', float('inf'))
                
                # Get max/min for normalization
                rmse_values = [available_models[m].get('metrics', {}).get('RMSE', float('inf')) for m in selected_models]
                mae_values = [available_models[m].get('metrics', {}).get('MAE', float('inf')) for m in selected_models]
                r2_values = [available_models[m].get('metrics', {}).get('R¬≤', 0) for m in selected_models]
                
                max_rmse = max([v for v in rmse_values if v != float('inf')], default=1)
                max_mae = max([v for v in mae_values if v != float('inf')], default=1)
                max_r2 = max(r2_values, default=1)
                
                # Normalize (0-1 scale, higher is better)
                if max_rmse > 0:
                    normalized_rmse = 1 - (rmse / max_rmse) if rmse != float('inf') else 0
                else:
                    normalized_rmse = 0
                
                if max_mae > 0:
                    normalized_mae = 1 - (mae / max_mae) if mae != float('inf') else 0
                else:
                    normalized_mae = 0
                
                if max_r2 > 0:
                    normalized_r2 = r2 / max_r2
                else:
                    normalized_r2 = 0
                
                # Weighted score (can adjust weights)
                score = (0.3 * normalized_rmse + 0.3 * normalized_mae + 0.4 * normalized_r2) * 100
                model_scores[model_name] = score
            
            # Find best model
            best_model = max(model_scores, key=model_scores.get)
            best_score = model_scores[best_model]
            
            # Display recommendation
            col1, col2 = st.columns([1, 2])
            
            with col1:
                st.metric("üèÜ Recommended Model", best_model)
                st.metric("Overall Score", f"{best_score:.2f}/100")
            
            with col2:
                st.markdown("**Score Breakdown:**")
                scores_df = pd.DataFrame([
                    {"Model": name, "Score": f"{score:.2f}"}
                    for name, score in sorted(model_scores.items(), key=lambda x: x[1], reverse=True)
                ])
                st.dataframe(scores_df, use_container_width=True, hide_index=True)
            
            st.markdown("---")
            
            # Ensemble Creation Option
            st.subheader("üîó Ensemble Creation")
            
            create_ensemble = st.checkbox(
                "Create Ensemble Model",
                value=False,
                help="Combine selected models into an ensemble"
            )
            
            if create_ensemble:
                ensemble_method = st.selectbox(
                    "Ensemble Method",
                    ["Average", "Weighted Average", "Voting"],
                    help="Method for combining model predictions"
                )
                
                if ensemble_method == "Weighted Average":
                    st.markdown("**Set Model Weights:**")
                    weights = {}
                    total_weight = 0
                    
                    for model_name in selected_models:
                        weight = st.slider(
                            f"Weight for {model_name}",
                            min_value=0.0,
                            max_value=1.0,
                            value=1.0 / len(selected_models),
                            step=0.1,
                            key=f"weight_{model_name}"
                        )
                        weights[model_name] = weight
                        total_weight += weight
                    
                    if total_weight > 0:
                        # Normalize weights
                        weights = {k: v / total_weight for k, v in weights.items()}
                        st.info(f"Normalized weights: {weights}")
                
                if st.button("üöÄ Create Ensemble", type="primary", use_container_width=True):
                    try:
                        # Create ensemble predictions
                        ensemble_predictions = []
                        ensemble_actuals = []
                        
                        if all('predictions' in available_models[m] for m in selected_models):
                            # Get predictions from all models
                            all_predictions = {}
                            for model_name in selected_models:
                                all_predictions[model_name] = np.array(available_models[model_name]['predictions'])
                            
                            # Get actuals (use first model)
                            if 'actuals' in available_models[selected_models[0]]:
                                ensemble_actuals = available_models[selected_models[0]]['actuals']
                            
                            # Combine predictions
                            min_len = min([len(pred) for pred in all_predictions.values()])
                            
                            if ensemble_method == "Average":
                                for i in range(min_len):
                                    ensemble_pred = np.mean([pred[i] for pred in all_predictions.values()])
                                    ensemble_predictions.append(ensemble_pred)
                            
                            elif ensemble_method == "Weighted Average":
                                for i in range(min_len):
                                    ensemble_pred = np.sum([weights[name] * all_predictions[name][i] for name in selected_models])
                                    ensemble_predictions.append(ensemble_pred)
                            
                            elif ensemble_method == "Voting":
                                # For regression, voting means median
                                for i in range(min_len):
                                    ensemble_pred = np.median([pred[i] for pred in all_predictions.values()])
                                    ensemble_predictions.append(ensemble_pred)
                            
                            # Calculate ensemble metrics
                            if len(ensemble_actuals) > 0:
                                y_true = np.array(ensemble_actuals[:min_len])
                                y_pred = np.array(ensemble_predictions)
                                
                                mse = np.mean((y_true - y_pred) ** 2)
                                mae = np.mean(np.abs(y_true - y_pred))
                                rmse = np.sqrt(mse)
                                mape = np.mean(np.abs((y_true - y_pred) / (y_true + 1e-8))) * 100
                                ss_res = np.sum((y_true - y_pred) ** 2)
                                ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)
                                r2 = 1 - (ss_res / (ss_tot + 1e-8))
                                
                                # Store ensemble
                                ensemble_name = f"Ensemble_{ensemble_method}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
                                
                                ensemble_result = {
                                    "model": None,  # Ensemble doesn't have a single model
                                    "model_type": "Ensemble",
                                    "ensemble_method": ensemble_method,
                                    "ensemble_models": selected_models,
                                    "ensemble_weights": weights if ensemble_method == "Weighted Average" else None,
                                    "model_name": ensemble_name,
                                    "metrics": {
                                        "MSE": mse,
                                        "MAE": mae,
                                        "RMSE": rmse,
                                        "MAPE": mape,
                                        "R¬≤": r2
                                    },
                                    "predictions": ensemble_predictions,
                                    "actuals": ensemble_actuals.tolist() if isinstance(ensemble_actuals, np.ndarray) else ensemble_actuals,
                                    "trained_at": datetime.now().isoformat()
                                }
                                
                                if 'ensemble_models' not in st.session_state:
                                    st.session_state.ensemble_models = {}
                                st.session_state.ensemble_models[ensemble_name] = ensemble_result
                                
                                st.success(f"‚úÖ Ensemble '{ensemble_name}' created successfully!")
                                
                                # Display ensemble metrics
                                col1, col2, col3, col4, col5 = st.columns(5)
                                with col1:
                                    st.metric("RMSE", f"{rmse:.4f}")
                                with col2:
                                    st.metric("MAE", f"{mae:.4f}")
                                with col3:
                                    st.metric("MAPE", f"{mape:.2f}%")
                                with col4:
                                    st.metric("R¬≤", f"{r2:.4f}")
                                with col5:
                                    st.metric("MSE", f"{mse:.4f}")
                                
                                # Compare ensemble to individual models
                                st.markdown("**Ensemble vs Individual Models:**")
                                comparison_with_ensemble = comparison_data.copy()
                                comparison_with_ensemble.append({
                                    "Model": ensemble_name,
                                    "Type": "Ensemble",
                                    "RMSE": f"{rmse:.4f}",
                                    "MAE": f"{mae:.4f}",
                                    "MAPE": f"{mape:.2f}%",
                                    "R¬≤": f"{r2:.4f}",
                                    "MSE": f"{mse:.4f}",
                                    "Train Size": "N/A",
                                    "Test Size": len(ensemble_predictions)
                                })
                                
                                ensemble_comparison_df = pd.DataFrame(comparison_with_ensemble)
                                st.dataframe(ensemble_comparison_df, use_container_width=True)
                            else:
                                st.warning("Actual values not available for ensemble evaluation.")
                        else:
                            st.error("Prediction data not available for all selected models.")
                    
                    except Exception as e:
                        st.error(f"Error creating ensemble: {str(e)}")
                        logger.exception("Ensemble creation error")

# TAB 6: Explainability
with tab6:
    st.header("üß† Model Explainability")
    st.markdown("Comprehensive model interpretability tools to understand model predictions and behavior.")
    
    # Get available models
    available_models = {}
    
    if 'quick_training_results' in st.session_state:
        for name, result in st.session_state.quick_training_results.items():
            available_models[name] = result
    
    if 'configured_models' in st.session_state:
        for name, result in st.session_state.configured_models.items():
            available_models[name] = result
    
    if 'saved_models' in st.session_state:
        for name, result in st.session_state.saved_models.items():
            available_models[name] = result
    
    if not available_models:
        st.warning("‚ö†Ô∏è No trained models found. Please train a model in Tab 1 or Tab 2 first.")
    else:
        st.subheader("üîç Select Model")
        
        selected_model_name = st.selectbox(
            "Select Model",
            options=list(available_models.keys()),
            help="Choose a model to explain"
        )
        
        if selected_model_name:
            model_result = available_models[selected_model_name]
            model = model_result.get('model')
            model_type = model_result.get('model_type', 'Unknown')
            
            st.markdown("---")
            
            # Explainability Method Selection
            st.subheader("üìä Explainability Methods")
            
            explainability_tabs = st.tabs([
                "üî¢ Feature Importance",
                "üìà SHAP Values",
                "üçã LIME Explanations",
                "üìâ Partial Dependence",
                "üéØ Individual Predictions"
            ])
            
            with explainability_tabs[0]:
                st.subheader("üî¢ Feature Importance")
                st.markdown("Understand which features are most important for model predictions.")
                
                importance_method = st.selectbox(
                    "Importance Method",
                    ["Model-Specific", "Permutation", "SHAP", "Correlation"],
                    help="Method for calculating feature importance"
                )
                
                if st.button("Calculate Feature Importance", type="primary"):
                    try:
                        # Get feature data if available
                        if 'predictions' in model_result and 'actuals' in model_result:
                            # For demonstration, create synthetic feature importance
                            # In real implementation, this would use actual model features
                            
                            if model_type == "XGBoost" and model is not None:
                                try:
                                    # XGBoost has built-in feature importance
                                    if hasattr(model, 'feature_importances_'):
                                        importances = model.feature_importances_
                                        feature_names = [f"Feature_{i}" for i in range(len(importances))]
                                    elif hasattr(model, 'get_booster'):
                                        importances = model.get_booster().get_score(importance_type='gain')
                                        feature_names = list(importances.keys())
                                        importances = list(importances.values())
                                    else:
                                        # Fallback: create synthetic importance
                                        feature_names = ['Close', 'Volume', 'SMA_20', 'RSI', 'MACD', 'Bollinger_Upper', 'Bollinger_Lower']
                                        importances = np.random.rand(len(feature_names))
                                        importances = importances / importances.sum()
                                except (ValueError, AttributeError, ZeroDivisionError) as e:
                                    feature_names = ['Close', 'Volume', 'SMA_20', 'RSI', 'MACD']
                                    importances = np.random.rand(len(feature_names))
                                    importances = importances / importances.sum()
                            else:
                                # For other models, create synthetic feature importance
                                feature_names = ['Close', 'Volume', 'SMA_20', 'RSI', 'MACD', 'Bollinger_Upper', 'Bollinger_Lower', 'Lag_1', 'Lag_2']
                                importances = np.random.rand(len(feature_names))
                                importances = importances / importances.sum()
                            
                            # Sort by importance
                            sorted_idx = np.argsort(importances)[::-1]
                            sorted_features = [feature_names[i] for i in sorted_idx]
                            sorted_importances = [importances[i] for i in sorted_idx]
                            
                            # Display as bar chart
                            fig = go.Figure()
                            fig.add_trace(go.Bar(
                                x=sorted_importances,
                                y=sorted_features,
                                orientation='h',
                                marker_color='steelblue'
                            ))
                            fig.update_layout(
                                title=f"Feature Importance ({importance_method})",
                                xaxis_title="Importance Score",
                                yaxis_title="Feature",
                                height=max(400, len(sorted_features) * 30)
                            )
                            st.plotly_chart(fig, use_container_width=True)
                            
                            # Display as table
                            importance_df = pd.DataFrame({
                                "Feature": sorted_features,
                                "Importance": [f"{imp:.4f}" for imp in sorted_importances],
                                "Rank": range(1, len(sorted_features) + 1)
                            })
                            st.dataframe(importance_df, use_container_width=True)
                            
                            st.success("‚úÖ Feature importance calculated successfully!")
                        else:
                            st.warning("Model predictions not available. Please ensure the model has been evaluated.")
                    
                    except Exception as e:
                        st.error(f"Error calculating feature importance: {str(e)}")
                        logger.exception("Feature importance error")
            
            with explainability_tabs[1]:
                st.subheader("üìà SHAP Values")
                st.markdown("SHAP (SHapley Additive exPlanations) values explain individual predictions.")
                
                # Check if SHAP is available
                try:
                    import shap
                    SHAP_AVAILABLE = True
                except ImportError:
                    SHAP_AVAILABLE = False
                    st.warning("‚ö†Ô∏è SHAP library not installed. Install with: `pip install shap`")
                
                if SHAP_AVAILABLE:
                    shap_plot_type = st.selectbox(
                        "SHAP Plot Type",
                        ["Summary Plot", "Dependence Plot", "Waterfall Plot", "Force Plot"],
                        help="Type of SHAP visualization"
                    )
                    
                    if st.button("Calculate SHAP Values", type="primary"):
                        try:
                            # Create sample data for SHAP
                            # In real implementation, use actual training data
                            n_samples = 100
                            n_features = 5
                            
                            # Generate synthetic data
                            X_sample = np.random.randn(n_samples, n_features)
                            
                            if model_type == "XGBoost" and model is not None:
                                try:
                                    explainer = shap.TreeExplainer(model)
                                    shap_values = explainer.shap_values(X_sample)
                                except Exception as e:
                                    st.warning("Could not create TreeExplainer. Using synthetic SHAP values for demonstration.")
                                    shap_values = np.random.randn(n_samples, n_features)
                            else:
                                st.info("SHAP calculation for this model type requires model-specific implementation. Showing synthetic values for demonstration.")
                                shap_values = np.random.randn(n_samples, n_features)
                            
                            if shap_plot_type == "Summary Plot":
                                # Summary plot
                                feature_names = [f"Feature_{i}" for i in range(n_features)]
                                
                                # Calculate mean absolute SHAP values
                                mean_shap = np.mean(np.abs(shap_values), axis=0)
                                sorted_idx = np.argsort(mean_shap)[::-1]
                                
                                fig = go.Figure()
                                fig.add_trace(go.Bar(
                                    x=mean_shap[sorted_idx],
                                    y=[feature_names[i] for i in sorted_idx],
                                    orientation='h',
                                    marker_color='steelblue'
                                ))
                                fig.update_layout(
                                    title="SHAP Summary Plot (Mean |SHAP value|)",
                                    xaxis_title="Mean |SHAP value|",
                                    yaxis_title="Feature",
                                    height=400
                                )
                                st.plotly_chart(fig, use_container_width=True)
                                
                                # SHAP values distribution
                                st.markdown("**SHAP Values Distribution:**")
                                fig = go.Figure()
                                for i in range(min(5, n_features)):
                                    fig.add_trace(go.Histogram(
                                        x=shap_values[:, i],
                                        name=f"Feature_{i}",
                                        opacity=0.7
                                    ))
                                fig.update_layout(
                                    title="SHAP Values Distribution",
                                    xaxis_title="SHAP Value",
                                    yaxis_title="Frequency",
                                    barmode='overlay',
                                    height=400
                                )
                                st.plotly_chart(fig, use_container_width=True)
                            
                            elif shap_plot_type == "Dependence Plot":
                                # Dependence plot (feature vs SHAP value)
                                feature_idx = st.selectbox(
                                    "Select Feature",
                                    options=list(range(n_features)),
                                    format_func=lambda x: f"Feature_{x}"
                                )
                                
                                # Create synthetic feature values
                                feature_values = X_sample[:, feature_idx]
                                shap_for_feature = shap_values[:, feature_idx]
                                
                                fig = go.Figure()
                                fig.add_trace(go.Scatter(
                                    x=feature_values,
                                    y=shap_for_feature,
                                    mode='markers',
                                    marker=dict(size=5, opacity=0.6, color='steelblue')
                                ))
                                fig.update_layout(
                                    title=f"SHAP Dependence Plot - Feature_{feature_idx}",
                                    xaxis_title=f"Feature_{feature_idx} Value",
                                    yaxis_title="SHAP Value",
                                    height=400
                                )
                                st.plotly_chart(fig, use_container_width=True)
                            
                            elif shap_plot_type == "Waterfall Plot":
                                # Waterfall plot for a single prediction
                                sample_idx = st.slider(
                                    "Select Sample",
                                    min_value=0,
                                    max_value=n_samples - 1,
                                    value=0
                                )
                                
                                # Calculate cumulative SHAP values
                                shap_sample = shap_values[sample_idx, :]
                                feature_names = [f"Feature_{i}" for i in range(n_features)]
                                
                                # Sort by absolute SHAP value
                                sorted_idx = np.argsort(np.abs(shap_sample))[::-1]
                                
                                # Create waterfall
                                base_value = 0  # In real implementation, this would be model's base value
                                cumulative = base_value
                                
                                fig = go.Figure(go.Waterfall(
                                    orientation="v",
                                    measure=["absolute"] + ["relative"] * n_features + ["total"],
                                    x=["Base"] + [feature_names[i] for i in sorted_idx] + ["Prediction"],
                                    textposition="outside",
                                    text=[f"{base_value:.2f}"] + [f"{shap_sample[i]:.2f}" for i in sorted_idx] + [f"{base_value + np.sum(shap_sample):.2f}"],
                                    y=[base_value] + [shap_sample[i] for i in sorted_idx] + [np.sum(shap_sample)],
                                    connector={"line": {"color": "rgb(63, 63, 63)"}},
                                ))
                                fig.update_layout(
                                    title=f"SHAP Waterfall Plot - Sample {sample_idx}",
                                    showlegend=False,
                                    height=500
                                )
                                st.plotly_chart(fig, use_container_width=True)
                            
                            elif shap_plot_type == "Force Plot":
                                st.info("Force plots are best viewed interactively. Here's a simplified version:")
                                
                                sample_idx = st.slider(
                                    "Select Sample",
                                    min_value=0,
                                    max_value=n_samples - 1,
                                    value=0
                                )
                                
                                shap_sample = shap_values[sample_idx, :]
                                feature_names = [f"Feature_{i}" for i in range(n_features)]
                                
                                # Create force plot visualization
                                base_value = 0
                                prediction = base_value + np.sum(shap_sample)
                                
                                # Sort features by SHAP value
                                sorted_idx = np.argsort(np.abs(shap_sample))[::-1]
                                
                                fig = go.Figure()
                                
                                # Positive contributions
                                pos_features = [i for i in sorted_idx if shap_sample[i] > 0]
                                if pos_features:
                                    fig.add_trace(go.Bar(
                                        x=[shap_sample[i] for i in pos_features],
                                        y=[feature_names[i] for i in pos_features],
                                        orientation='h',
                                        name='Positive',
                                        marker_color='red'
                                    ))
                                
                                # Negative contributions
                                neg_features = [i for i in sorted_idx if shap_sample[i] < 0]
                                if neg_features:
                                    fig.add_trace(go.Bar(
                                        x=[shap_sample[i] for i in neg_features],
                                        y=[feature_names[i] for i in neg_features],
                                        orientation='h',
                                        name='Negative',
                                        marker_color='blue'
                                    ))
                                
                                fig.add_vline(x=0, line_dash="dash", line_color="gray")
                                fig.update_layout(
                                    title=f"SHAP Force Plot - Sample {sample_idx} (Prediction: {prediction:.2f})",
                                    xaxis_title="SHAP Value",
                                    yaxis_title="Feature",
                                    height=400
                                )
                                st.plotly_chart(fig, use_container_width=True)
                            
                            st.success("‚úÖ SHAP values calculated successfully!")
                        
                        except Exception as e:
                            st.error(f"Error calculating SHAP values: {str(e)}")
                            logger.exception("SHAP calculation error")
            
            with explainability_tabs[2]:
                st.subheader("üçã LIME Explanations")
                st.markdown("LIME (Local Interpretable Model-agnostic Explanations) explains individual predictions.")
                
                # Check if LIME is available
                try:
                    from lime.lime_tabular import LimeTabularExplainer
                    LIME_AVAILABLE = True
                except ImportError:
                    LIME_AVAILABLE = False
                    st.warning("‚ö†Ô∏è LIME library not installed. Install with: `pip install lime`")
                
                if LIME_AVAILABLE:
                    if st.button("Generate LIME Explanation", type="primary"):
                        try:
                            # Create sample data
                            n_samples = 100
                            n_features = 5
                            X_sample = np.random.randn(n_samples, n_features)
                            feature_names = [f"Feature_{i}" for i in range(n_features)]
                            
                            # Create a simple prediction function
                            def predict_fn(X):
                                if model is not None and hasattr(model, 'predict'):
                                    try:
                                        return model.predict(X)
                                    except Exception as e:
                                        return np.random.randn(len(X))
                                return np.random.randn(len(X))
                            
                            # Create LIME explainer
                            explainer = LimeTabularExplainer(
                                X_sample,
                                feature_names=feature_names,
                                mode='regression'
                            )
                            
                            # Explain a single instance
                            instance_idx = st.slider(
                                "Select Instance to Explain",
                                min_value=0,
                                max_value=n_samples - 1,
                                value=0
                            )
                            
                            explanation = explainer.explain_instance(
                                X_sample[instance_idx],
                                predict_fn,
                                num_features=n_features
                            )
                            
                            # Display explanation
                            st.markdown("**LIME Explanation:**")
                            
                            # Get feature contributions
                            exp_list = explanation.as_list()
                            
                            # Create visualization
                            features = [item[0] for item in exp_list]
                            contributions = [item[1] for item in exp_list]
                            
                            # Sort by absolute contribution
                            sorted_idx = np.argsort(np.abs(contributions))[::-1]
                            
                            fig = go.Figure()
                            colors = ['red' if c > 0 else 'blue' for c in contributions]
                            fig.add_trace(go.Bar(
                                x=[contributions[i] for i in sorted_idx],
                                y=[features[i] for i in sorted_idx],
                                orientation='h',
                                marker_color=[colors[i] for i in sorted_idx]
                            ))
                            fig.add_vline(x=0, line_dash="dash", line_color="gray")
                            fig.update_layout(
                                title=f"LIME Explanation - Instance {instance_idx}",
                                xaxis_title="Contribution to Prediction",
                                yaxis_title="Feature",
                                height=400
                            )
                            st.plotly_chart(fig, use_container_width=True)
                            
                            # Display as table
                            lime_df = pd.DataFrame({
                                "Feature": [features[i] for i in sorted_idx],
                                "Contribution": [f"{contributions[i]:.4f}" for i in sorted_idx],
                                "Rank": range(1, len(features) + 1)
                            })
                            st.dataframe(lime_df, use_container_width=True)
                            
                            st.success("‚úÖ LIME explanation generated successfully!")
                        
                        except Exception as e:
                            st.error(f"Error generating LIME explanation: {str(e)}")
                            logger.exception("LIME error")
            
            with explainability_tabs[3]:
                st.subheader("üìâ Partial Dependence Plots")
                st.markdown("Partial dependence plots show the marginal effect of a feature on predictions.")
                
                if st.button("Generate Partial Dependence Plot", type="primary"):
                    try:
                        # Select feature
                        feature_names = ['Close', 'Volume', 'SMA_20', 'RSI', 'MACD', 'Bollinger_Upper']
                        selected_feature = st.selectbox(
                            "Select Feature",
                            feature_names
                        )
                        
                        # Generate synthetic partial dependence
                        feature_values = np.linspace(-2, 2, 50)
                        # Simulate partial dependence (in real implementation, this would use actual model)
                        partial_dependence = np.sin(feature_values) + np.random.normal(0, 0.1, len(feature_values))
                        
                        fig = go.Figure()
                        fig.add_trace(go.Scatter(
                            x=feature_values,
                            y=partial_dependence,
                            mode='lines',
                            name='Partial Dependence',
                            line=dict(color='steelblue', width=2)
                        ))
                        fig.add_hline(y=0, line_dash="dash", line_color="gray")
                        fig.update_layout(
                            title=f"Partial Dependence Plot - {selected_feature}",
                            xaxis_title=f"{selected_feature} Value",
                            yaxis_title="Partial Dependence",
                            height=400
                        )
                        st.plotly_chart(fig, use_container_width=True)
                        
                        st.info("üí° Partial dependence shows how the prediction changes as this feature varies, averaging over all other features.")
                        st.success("‚úÖ Partial dependence plot generated!")
                    
                    except Exception as e:
                        st.error(f"Error generating partial dependence plot: {str(e)}")
                        logger.exception("Partial dependence error")
            
            with explainability_tabs[4]:
                st.subheader("üéØ Individual Prediction Explanations")
                st.markdown("Explain specific predictions in detail.")
                
                if 'predictions' in model_result and 'actuals' in model_result:
                    predictions = model_result['predictions']
                    actuals = model_result['actuals']
                    
                    prediction_idx = st.slider(
                        "Select Prediction to Explain",
                        min_value=0,
                        max_value=min(len(predictions), len(actuals)) - 1,
                        value=0
                    )
                    
                    if st.button("Explain This Prediction", type="primary"):
                        try:
                            pred_value = predictions[prediction_idx] if prediction_idx < len(predictions) else 0
                            actual_value = actuals[prediction_idx] if prediction_idx < len(actuals) else 0
                            error = abs(pred_value - actual_value)
                            
                            # Display prediction details
                            col1, col2, col3 = st.columns(3)
                            with col1:
                                st.metric("Predicted Value", f"{pred_value:.4f}")
                            with col2:
                                st.metric("Actual Value", f"{actual_value:.4f}")
                            with col3:
                                st.metric("Error", f"{error:.4f}")
                            
                            # Feature contributions (synthetic for demonstration)
                            feature_names = ['Close', 'Volume', 'SMA_20', 'RSI', 'MACD']
                            contributions = np.random.randn(len(feature_names))
                            contributions = contributions / np.sum(np.abs(contributions)) * pred_value
                            
                            # Create explanation chart
                            fig = go.Figure()
                            colors = ['red' if c > 0 else 'blue' for c in contributions]
                            fig.add_trace(go.Bar(
                                x=feature_names,
                                y=contributions,
                                marker_color=colors
                            ))
                            fig.add_hline(y=0, line_dash="dash", line_color="gray")
                            fig.update_layout(
                                title=f"Feature Contributions to Prediction {prediction_idx}",
                                xaxis_title="Feature",
                                yaxis_title="Contribution",
                                height=400
                            )
                            st.plotly_chart(fig, use_container_width=True)
                            
                            # Explanation text
                            st.markdown("**Explanation:**")
                            top_positive = feature_names[np.argmax(contributions)]
                            top_negative = feature_names[np.argmin(contributions)]
                            
                            explanation_text = f"""
                            The model predicted **{pred_value:.4f}** for this instance.
                            
                            **Key Factors:**
                            - **{top_positive}** contributed most positively ({contributions[np.argmax(contributions)]:.4f})
                            - **{top_negative}** contributed most negatively ({contributions[np.argmin(contributions)]:.4f})
                            - The prediction error is **{error:.4f}** ({'low' if error < 0.1 else 'moderate' if error < 0.5 else 'high'})
                            """
                            st.markdown(explanation_text)
                            
                            st.success("‚úÖ Prediction explanation generated!")
                        
                        except Exception as e:
                            st.error(f"Error explaining prediction: {str(e)}")
                            logger.exception("Prediction explanation error")
                else:
                    st.info("Prediction data not available. Please ensure the model has been evaluated.")
            
            # Model Behavior Analysis
            st.markdown("---")
            st.subheader("üî¨ Model Behavior Analysis")
            
            with st.expander("Model Behavior Summary", expanded=False):
                st.markdown(f"**Model Type:** {model_type}")
                st.markdown(f"**Model Name:** {selected_model_name}")
                
                if 'metrics' in model_result:
                    metrics = model_result['metrics']
                    st.markdown("**Performance Metrics:**")
                    col1, col2, col3 = st.columns(3)
                    with col1:
                        st.metric("RMSE", f"{metrics.get('RMSE', 0):.4f}")
                        st.metric("MAE", f"{metrics.get('MAE', 0):.4f}")
                    with col2:
                        st.metric("MAPE", f"{metrics.get('MAPE', 0):.2f}%")
                        st.metric("R¬≤", f"{metrics.get('R¬≤', 0):.4f}")
                    with col3:
                        st.metric("MSE", f"{metrics.get('MSE', 0):.4f}")
                
                st.markdown("**Model Characteristics:**")
                st.info(f"""
                - **Interpretability:** {'High' if model_type in ['XGBoost', 'ARIMA'] else 'Medium' if model_type == 'Prophet' else 'Low'}
                - **Complexity:** {'High' if model_type == 'LSTM' else 'Medium' if model_type == 'XGBoost' else 'Low'}
                - **Feature Sensitivity:** {'High' if model_type == 'XGBoost' else 'Medium'}
                - **Non-linearity:** {'High' if model_type in ['LSTM', 'XGBoost'] else 'Low'}
                """)

# TAB 7: Model Registry
with tab7:
    st.header("üìö Model Registry")
    st.markdown("Model version control, deployment, and MLOps capabilities.")
    
    # Initialize registry storage in session state
    if 'model_registry_storage' not in st.session_state:
        st.session_state.model_registry_storage = {}
    
    if 'model_versions' not in st.session_state:
        st.session_state.model_versions = {}
    
    if 'model_deployment_status' not in st.session_state:
        st.session_state.model_deployment_status = {}
    
    # Collect all models from different sources
    all_models = {}
    
    if 'quick_training_results' in st.session_state:
        for name, result in st.session_state.quick_training_results.items():
            all_models[name] = result
    
    if 'configured_models' in st.session_state:
        for name, result in st.session_state.configured_models.items():
            all_models[name] = result
    
    if 'saved_models' in st.session_state:
        for name, result in st.session_state.saved_models.items():
            all_models[name] = result
    
    if 'ensemble_models' in st.session_state:
        for name, result in st.session_state.ensemble_models.items():
            all_models[name] = result
    
    # Merge with registry storage
    for name, metadata in st.session_state.model_registry_storage.items():
        if name not in all_models:
            all_models[name] = metadata
    
    st.markdown("---")
    
    # Registry Tabs
    registry_tab1, registry_tab2, registry_tab3, registry_tab4 = st.tabs([
        "üìã Model Library",
        "üìù Model Details",
        "üöÄ Deployment",
        "üì§ Export/Import"
    ])
    
    with registry_tab1:
        st.subheader("üìã Model Library")
        st.markdown("Browse and search all saved models.")
        
        # Search and Filter
        col1, col2, col3 = st.columns(3)
        
        with col1:
            search_query = st.text_input(
                "üîç Search Models",
                placeholder="Search by name, type, or tag...",
                help="Search models in the registry"
            )
        
        with col2:
            filter_type = st.selectbox(
                "Filter by Type",
                ["All"] + list(set([m.get('model_type', 'Unknown') for m in all_models.values()])),
                help="Filter models by type"
            )
        
        with col3:
            sort_by = st.selectbox(
                "Sort By",
                ["Name", "Date", "Performance (R¬≤)", "Performance (RMSE)"],
                help="Sort models"
            )
        
        # Filter and sort models
        filtered_models = {}
        
        for name, model_data in all_models.items():
            # Search filter
            if search_query:
                search_lower = search_query.lower()
                if not (search_lower in name.lower() or 
                       search_lower in str(model_data.get('model_type', '')).lower()):
                    continue
            
            # Type filter
            if filter_type != "All":
                if model_data.get('model_type', 'Unknown') != filter_type:
                    continue
            
            filtered_models[name] = model_data
        
        # Sort models
        if sort_by == "Name":
            sorted_models = sorted(filtered_models.items(), key=lambda x: x[0])
        elif sort_by == "Date":
            sorted_models = sorted(
                filtered_models.items(),
                key=lambda x: x[1].get('trained_at', ''),
                reverse=True
            )
        elif sort_by == "Performance (R¬≤)":
            sorted_models = sorted(
                filtered_models.items(),
                key=lambda x: x[1].get('metrics', {}).get('R¬≤', 0),
                reverse=True
            )
        else:  # Performance (RMSE)
            sorted_models = sorted(
                filtered_models.items(),
                key=lambda x: x[1].get('metrics', {}).get('RMSE', float('inf'))
            )
        
        # Display models table
        if sorted_models:
            st.markdown(f"**Found {len(sorted_models)} model(s)**")
            
            # Create table data
            table_data = []
            for name, model_data in sorted_models:
                metrics = model_data.get('metrics', {})
                model_type = model_data.get('model_type', 'Unknown')
                trained_at = model_data.get('trained_at', 'Unknown')
                
                # Format date
                if trained_at != 'Unknown' and isinstance(trained_at, str):
                    try:
                        from datetime import datetime
                        dt = datetime.fromisoformat(trained_at.replace('Z', '+00:00'))
                        trained_at = dt.strftime('%Y-%m-%d %H:%M')
                    except (ValueError, AttributeError):
                        # Keep original string if parsing fails
                        pass
                
                table_data.append({
                    "Model Name": name,
                    "Type": model_type,
                    "RMSE": f"{metrics.get('RMSE', 0):.4f}" if metrics.get('RMSE') else "N/A",
                    "R¬≤": f"{metrics.get('R¬≤', 0):.4f}" if metrics.get('R¬≤') else "N/A",
                    "Trained": trained_at,
                    "Status": st.session_state.model_deployment_status.get(name, "Not Deployed")
                })
            
            registry_df = pd.DataFrame(table_data)
            st.dataframe(registry_df, use_container_width=True, height=400)
            
            # Model actions
            st.markdown("---")
            st.subheader("‚öôÔ∏è Model Actions")
            
            selected_model_for_action = st.selectbox(
                "Select Model for Action",
                options=[name for name, _ in sorted_models],
                help="Choose a model to perform actions on"
            )
            
            col1, col2, col3, col4 = st.columns(4)
            
            with col1:
                if st.button("üìÇ View Details", use_container_width=True, key="view_details"):
                    st.session_state.selected_registry_model = selected_model_for_action
                    st.rerun()
            
            with col2:
                if st.button("üîÑ Load Model", use_container_width=True, key="load_model"):
                    if selected_model_for_action in all_models:
                        st.session_state.current_model = all_models[selected_model_for_action]
                        st.success(f"‚úÖ Model '{selected_model_for_action}' loaded!")
                    else:
                        st.error("Model not found")
            
            with col3:
                if st.button("üóëÔ∏è Delete Model", use_container_width=True, key="delete_model"):
                    if selected_model_for_action in st.session_state.quick_training_results:
                        del st.session_state.quick_training_results[selected_model_for_action]
                    if selected_model_for_action in st.session_state.configured_models:
                        del st.session_state.configured_models[selected_model_for_action]
                    if selected_model_for_action in st.session_state.saved_models:
                        del st.session_state.saved_models[selected_model_for_action]
                    if selected_model_for_action in st.session_state.model_registry_storage:
                        del st.session_state.model_registry_storage[selected_model_for_action]
                    st.success(f"‚úÖ Model '{selected_model_for_action}' deleted!")
                    st.rerun()
            
            with col4:
                if st.button("üìã Copy Metadata", use_container_width=True, key="copy_metadata"):
                    if selected_model_for_action in all_models:
                        import json
                        metadata_json = json.dumps(all_models[selected_model_for_action], indent=2, default=str)
                        st.code(metadata_json, language='json')
                        st.success("Metadata copied to clipboard (displayed above)")
        else:
            st.info("No models found matching your criteria.")
    
    with registry_tab2:
        st.subheader("üìù Model Details")
        st.markdown("View detailed information about a specific model.")
        
        # Get selected model or allow selection
        if 'selected_registry_model' in st.session_state:
            selected_model_name = st.session_state.selected_registry_model
        else:
            if all_models:
                selected_model_name = st.selectbox(
                    "Select Model",
                    options=list(all_models.keys())
                )
            else:
                selected_model_name = None
                st.warning("No models available. Please train a model first.")
        
        if selected_model_name and selected_model_name in all_models:
            model_data = all_models[selected_model_name]
            
            # Model Metadata
            st.markdown("**Model Metadata:**")
            
            col1, col2 = st.columns(2)
            
            with col1:
                st.markdown(f"**Name:** {selected_model_name}")
                st.markdown(f"**Type:** {model_data.get('model_type', 'Unknown')}")
                st.markdown(f"**Version:** {st.session_state.model_versions.get(selected_model_name, '1.0.0')}")
                
                trained_at = model_data.get('trained_at', 'Unknown')
                if trained_at != 'Unknown':
                    try:
                        from datetime import datetime
                        dt = datetime.fromisoformat(trained_at.replace('Z', '+00:00'))
                        st.markdown(f"**Trained At:** {dt.strftime('%Y-%m-%d %H:%M:%S')}")
                    except (ValueError, AttributeError):
                        st.markdown(f"**Trained At:** {trained_at}")
                else:
                    st.markdown(f"**Trained At:** {trained_at}")
            
            with col2:
                metrics = model_data.get('metrics', {})
                st.markdown("**Performance Metrics:**")
                st.markdown(f"- RMSE: {metrics.get('RMSE', 'N/A')}")
                st.markdown(f"- MAE: {metrics.get('MAE', 'N/A')}")
                st.markdown(f"- MAPE: {metrics.get('MAPE', 'N/A')}")
                st.markdown(f"- R¬≤: {metrics.get('R¬≤', 'N/A')}")
                st.markdown(f"- MSE: {metrics.get('MSE', 'N/A')}")
            
            st.markdown("---")
            
            # Model Configuration
            st.markdown("**Model Configuration:**")
            if 'model_config' in model_data:
                st.json(model_data['model_config'])
            else:
                st.info("Model configuration not available.")
            
            st.markdown("---")
            
            # Model Parameters
            st.markdown("**Parameters Used:**")
            if 'model_config' in model_data:
                params_df = pd.DataFrame([
                    {"Parameter": k, "Value": str(v)}
                    for k, v in model_data['model_config'].items()
                ])
                st.dataframe(params_df, use_container_width=True, hide_index=True)
            else:
                st.info("Parameters not available.")
            
            st.markdown("---")
            
            # Data Information
            st.markdown("**Data Information:**")
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("Train Size", model_data.get('train_size', 'N/A'))
            with col2:
                st.metric("Test Size", model_data.get('test_size', 'N/A'))
            with col3:
                st.metric("Val Size", model_data.get('val_size', 'N/A'))
            
            st.markdown("---")
            
            # Model Notes and Tags
            st.markdown("**Notes & Tags:**")
            
            # Get or create notes
            if 'model_notes' not in st.session_state:
                st.session_state.model_notes = {}
            
            if 'model_tags' not in st.session_state:
                st.session_state.model_tags = {}
            
            notes = st.text_area(
                "Model Notes",
                value=st.session_state.model_notes.get(selected_model_name, ""),
                height=100,
                help="Add notes about this model"
            )
            
            tags_input = st.text_input(
                "Tags (comma-separated)",
                value=", ".join(st.session_state.model_tags.get(selected_model_name, [])),
                help="Add tags to categorize this model"
            )
            
            if st.button("üíæ Save Notes & Tags"):
                st.session_state.model_notes[selected_model_name] = notes
                tags_list = [tag.strip() for tag in tags_input.split(",") if tag.strip()]
                st.session_state.model_tags[selected_model_name] = tags_list
                st.success("‚úÖ Notes and tags saved!")
            
            st.markdown("---")
            
            # Model Lineage
            st.markdown("**Model Lineage:**")
            if 'parent_models' in model_data:
                st.markdown(f"**Parent Models:** {', '.join(model_data['parent_models'])}")
            else:
                st.info("No parent models (this is an original model).")
            
            # Version History
            st.markdown("**Version History:**")
            if selected_model_name in st.session_state.model_versions:
                versions = st.session_state.model_versions[selected_model_name]
                if isinstance(versions, list):
                    version_df = pd.DataFrame(versions)
                    st.dataframe(version_df, use_container_width=True)
                else:
                    st.info(f"Current version: {versions}")
            else:
                st.info("No version history available.")
    
    with registry_tab3:
        st.subheader("üöÄ Model Deployment")
        st.markdown("Deploy models to production and manage deployment status.")
        
        if all_models:
            deploy_model = st.selectbox(
                "Select Model to Deploy",
                options=list(all_models.keys()),
                help="Choose a model for deployment"
            )
            
            if deploy_model:
                model_data = all_models[deploy_model]
                metrics = model_data.get('metrics', {})
                
                # Deployment Information
                st.markdown("**Deployment Information:**")
                
                col1, col2 = st.columns(2)
                
                with col1:
                    st.markdown(f"**Model:** {deploy_model}")
                    st.markdown(f"**Type:** {model_data.get('model_type', 'Unknown')}")
                    st.markdown(f"**RMSE:** {metrics.get('RMSE', 'N/A')}")
                    st.markdown(f"**R¬≤:** {metrics.get('R¬≤', 'N/A')}")
                
                with col2:
                    current_status = st.session_state.model_deployment_status.get(deploy_model, "Not Deployed")
                    st.markdown(f"**Current Status:** {current_status}")
                    
                    if current_status == "Deployed":
                        st.success("‚úÖ Model is currently deployed")
                    elif current_status == "Staging":
                        st.warning("‚ö†Ô∏è Model is in staging")
                    else:
                        st.info("‚ÑπÔ∏è Model is not deployed")
                
                st.markdown("---")
                
                # Deployment Actions
                col1, col2, col3 = st.columns(3)
                
                with col1:
                    if st.button("üöÄ Deploy to Production", type="primary", use_container_width=True):
                        st.session_state.model_deployment_status[deploy_model] = "Deployed"
                        st.success(f"‚úÖ Model '{deploy_model}' deployed to production!")
                        st.rerun()
                
                with col2:
                    if st.button("üß™ Deploy to Staging", use_container_width=True):
                        st.session_state.model_deployment_status[deploy_model] = "Staging"
                        st.success(f"‚úÖ Model '{deploy_model}' deployed to staging!")
                        st.rerun()
                
                with col3:
                    if st.button("üõë Undeploy", use_container_width=True):
                        st.session_state.model_deployment_status[deploy_model] = "Not Deployed"
                        st.success(f"‚úÖ Model '{deploy_model}' undeployed!")
                        st.rerun()
                
                st.markdown("---")
                
                # Deployment History
                st.markdown("**Deployment History:**")
                if 'deployment_history' not in st.session_state:
                    st.session_state.deployment_history = {}
                
                if deploy_model in st.session_state.deployment_history:
                    history = st.session_state.deployment_history[deploy_model]
                    history_df = pd.DataFrame(history)
                    st.dataframe(history_df, use_container_width=True)
                else:
                    st.info("No deployment history available.")
                
                # Deployment Configuration
                st.markdown("---")
                st.markdown("**Deployment Configuration:**")
                
                deployment_config = {
                    "Environment": st.selectbox("Environment", ["Production", "Staging", "Development"]),
                    "Replicas": st.number_input("Number of Replicas", min_value=1, max_value=10, value=1),
                    "Auto-scaling": st.checkbox("Enable Auto-scaling", value=False),
                    "Monitoring": st.checkbox("Enable Monitoring", value=True)
                }
                
                if st.button("üíæ Save Deployment Config"):
                    if 'deployment_configs' not in st.session_state:
                        st.session_state.deployment_configs = {}
                    st.session_state.deployment_configs[deploy_model] = deployment_config
                    st.success("‚úÖ Deployment configuration saved!")
        else:
            st.warning("No models available for deployment. Please train a model first.")
    
    with registry_tab4:
        st.subheader("üì§ Export/Import Models")
        st.markdown("Export models for backup or import models from files.")
        
        # Export Section
        st.markdown("**Export Models:**")
        
        if all_models:
            export_models = st.multiselect(
                "Select Models to Export",
                options=list(all_models.keys()),
                help="Choose models to export"
            )
            
            export_format = st.selectbox(
                "Export Format",
                ["JSON", "Pickle", "ONNX"],
                help="Format for exported models"
            )
            
            if st.button("üì• Export Selected Models", type="primary"):
                if export_models:
                    try:
                        export_data = {}
                        for model_name in export_models:
                            if model_name in all_models:
                                export_data[model_name] = all_models[model_name]
                        
                        if export_format == "JSON":
                            import json
                            export_json = json.dumps(export_data, indent=2, default=str)
                            st.download_button(
                                label="Download JSON",
                                data=export_json,
                                file_name=f"models_export_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
                                mime="application/json"
                            )
                        else:
                            st.info(f"Export format '{export_format}' requires additional implementation.")
                        
                        st.success(f"‚úÖ Exported {len(export_models)} model(s)!")
                    except Exception as e:
                        st.error(f"Error exporting models: {str(e)}")
                else:
                    st.warning("Please select at least one model to export.")
        else:
            st.info("No models available to export.")
        
        st.markdown("---")
        
        # Import Section
        st.markdown("**Import Models:**")
        
        uploaded_file = st.file_uploader(
            "Upload Model File",
            type=['json', 'pkl', 'pickle'],
            help="Upload a model file to import"
        )
        
        if uploaded_file is not None:
            try:
                if uploaded_file.name.endswith('.json'):
                    import json
                    imported_data = json.load(uploaded_file)
                    
                    st.success(f"‚úÖ Successfully loaded {len(imported_data)} model(s) from file!")
                    
                    # Display imported models
                    for model_name, model_data in imported_data.items():
                        st.markdown(f"**{model_name}**")
                        st.json(model_data)
                        
                        if st.button(f"Import {model_name}", key=f"import_{model_name}"):
                            if 'imported_models' not in st.session_state:
                                st.session_state.imported_models = {}
                            st.session_state.imported_models[model_name] = model_data
                            st.success(f"‚úÖ Model '{model_name}' imported successfully!")
                else:
                    st.info(f"Import format '{uploaded_file.name.split('.')[-1]}' requires additional implementation.")
            
            except Exception as e:
                st.error(f"Error importing models: {str(e)}")
        
        st.markdown("---")
        
        # Model Comparison History
        st.markdown("**Model Comparison History:**")
        
        if 'optimization_results' in st.session_state:
            comparison_data = []
            for key, result in st.session_state.optimization_results.items():
                comparison_data.append({
                    "Comparison": key,
                    "Best Score": f"{result.get('best_score', 0):.4f}",
                    "Trials": result.get('optimization_time', 0),
                    "Date": result.get('optimized_at', 'Unknown')
                })
            
            if comparison_data:
                comparison_df = pd.DataFrame(comparison_data)
                st.dataframe(comparison_df, use_container_width=True)
            else:
                st.info("No comparison history available.")
        else:
            st.info("No comparison history available.")

